{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "- Reminder\n",
    "- Computing Hypothesis\n",
    "- Computing Cost Fucntion\n",
    "- Evaluataion\n",
    "- Higher Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder : Logistic Regression\n",
    "\n",
    "Logistic Regression은 binary classification 문제입니다.. 즉 우리에게 m개의 셈플을 가지고 d의 디맨션을 가진 x가 있다고 생각해 봅니다. \n",
    "그러면 (1,d) vector가 m개의 sample이 들어있다고도 볼수 있습니다.   \n",
    "\n",
    "[  \n",
    "[7,8,8,9,5,4],  \n",
    "[3,8,8,6,5,4],  \n",
    "[6,6,7,9,7,4],  \n",
    "[3,8,8,6,5,4],  \n",
    "[1,8,8,6,5,4],  \n",
    "]                  \n",
    "과 같은 matrix가 \n",
    "(m,1)크기의 어떠한\n",
    "[  \n",
    "[0],  \n",
    "[1],  \n",
    "[1],  \n",
    "[0],  \n",
    "[1]  \n",
    "]   \n",
    "에 대응된다고 생각하면 됩니다. \n",
    "즉 d차원의 (1,d)Vector가 주어졌을때 0또는 1 어디에 가까운지 구하는것이 Logistic Regression 입니다.  \n",
    "여기서 어떤값이 1일 확률 P(x=1)은 1-P(x=0) 이 되는것은 자명합니다.  \n",
    "\n",
    "자 이제 우리가 구할 W(Weight Parmeter)를 살펴보겠습니다.  \n",
    "W는 (d,1)크기의 Vector가 됩니다.  \n",
    "\n",
    "\n",
    "그리고 마지막으로 우리는 WX를 곱한값에 sigmoid함수를 사용해 0,1과 근사하게 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.FloatTensor([  \n",
    "[7,8,8,9,5,4],  \n",
    "[3,8,8,6,5,4],  \n",
    "[6,6,7,9,7,4],  \n",
    "[3,8,8,6,5,4],  \n",
    "[1,8,8,6,5,4],  \n",
    "])\n",
    "\n",
    "Y = torch.FloatTensor([  \n",
    "[0],  \n",
    "[1],  \n",
    "[1],  \n",
    "[0],  \n",
    "[1]  \n",
    "])\n",
    "\n",
    "# 이제 mul(X,W) = Y 에 대응하는 W 를 찾는것이 목적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 한 예제들을 수식으로 살펴 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "\n",
    "H(X) = 1/1+sigmoid(-W.T*X)  \n",
    "= P(X=1;W) # W가 주어졌을때 X=1일 값  \n",
    "= 1-P(X=0;W) 로도 표현 가능합니다.\n",
    "\n",
    "### Cost\n",
    "\n",
    "-1/M * mean(y*log(H(X)) + (1-y)(log(1-H(X))\n",
    "\n",
    "- if y ~= H(x), cost is near0\n",
    "- if y not H(x), cost is high\n",
    "\n",
    "### Weight Update via Gradient Descent\n",
    "W := W -a(d/dW)*cost(W)\n",
    "\n",
    "a : Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x170d4e51fb0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For repoducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "x_data = torch.FloatTensor([  \n",
    "[7,8,8,9,5,4],  \n",
    "[3,8,8,6,5,4],  \n",
    "[6,6,7,9,7,4],  \n",
    "[3,8,8,6,5,4],  \n",
    "[1,8,8,6,5,4],  \n",
    "])\n",
    "\n",
    "y_data = torch.FloatTensor([  \n",
    "[0],  \n",
    "[1],  \n",
    "[1],  \n",
    "[0],  \n",
    "[1]  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 6])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_data.shape) # m=5 d=6\n",
    "print(y_data.shape) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Hypothesis\n",
    "\n",
    "Pytorch has a torch.exp() function that resembles the exponential function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e^1 equals : tensor([2.7183])\n"
     ]
    }
   ],
   "source": [
    "print('e^1 equals :', torch.exp(torch.FloatTensor([1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4224],\n",
       "        [ 0.2673],\n",
       "        [-0.4212],\n",
       "        [-0.5107],\n",
       "        [-1.5727],\n",
       "        [-0.1232]], requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn((6,1), requires_grad=True)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.5870], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.randn(1, requires_grad=True)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = 1 / (1 + torch.exp(-(x_data.matmul(W) + b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.8072e-04],\n",
       "        [4.1067e-04],\n",
       "        [1.2116e-05],\n",
       "        [4.1067e-04],\n",
       "        [1.7648e-04]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch 에서는 torch.sigmoid를 제공하기 때문에 위의 계산을 구지 할 필요가 없다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7311])\n"
     ]
    }
   ],
   "source": [
    "print(torch.sigmoid(torch.FloatTensor([1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = torch.sigmoid(x_data.matmul(W)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.8072e-04],\n",
      "        [4.1067e-04],\n",
      "        [1.2116e-05],\n",
      "        [4.1067e-04],\n",
      "        [1.7648e-04]], grad_fn=<SigmoidBackward>)\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(hypothesis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Cost Function\n",
    "\n",
    "지금까지 H(x)를 구하는 과정을 살펴 보았습니다.  \n",
    "그럼 우리는 이 값과 y_data의 차이를 살펴보고 이를 통해 cost를 구할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = -(y_data * torch.log(hypothesis) + \n",
    "           (1 - y_data) * torch.log(1-hypothesis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.8083e-04],\n",
      "        [7.7977e+00],\n",
      "        [1.1321e+01],\n",
      "        [4.1076e-04],\n",
      "        [8.6423e+00]], grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5524, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = losses.mean() #평균을 취해줍니다.\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 지금까지 위에 식을  torch.nn.functional의 binary_cross_entropy를 이용하면 해결이 가능합니다. \n",
    "\n",
    "- cross entropy 인데 binary한 경우\n",
    "- cross entropy란? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.5524, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy(hypothesis,y_data) #bce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole Training Procedure\n",
    "\n",
    "전체 코드를 통해 학습을 해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 4.426760\n",
      "Epoch  100/1000 Cost: 2.220122\n",
      "Epoch  200/1000 Cost: 0.001048\n",
      "Epoch  300/1000 Cost: 0.000995\n",
      "Epoch  400/1000 Cost: 0.000959\n",
      "Epoch  500/1000 Cost: 0.000932\n",
      "Epoch  600/1000 Cost: 0.000909\n",
      "Epoch  700/1000 Cost: 0.000889\n",
      "Epoch  800/1000 Cost: 0.000872\n",
      "Epoch  900/1000 Cost: 0.000855\n"
     ]
    }
   ],
   "source": [
    "# Training Data\n",
    "x_train = torch.FloatTensor([  \n",
    "[7,8,8,9,5,4],  \n",
    "[3,8,4,6,5,0],  \n",
    "[6,6,7,6,7,4],  \n",
    "[3,8,2,1,6,2],  \n",
    "[1,8,2,6,6,4],  \n",
    "])\n",
    "\n",
    "y_train = torch.FloatTensor([  \n",
    "[0],  \n",
    "[1],  \n",
    "[1],  \n",
    "[0],  \n",
    "[1]  \n",
    "])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.randn((6,1), requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([W,b], lr=0.3) #SGC를 이용해서 W,b를 학습할꺼다\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs) :\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W)+b)\n",
    "    cost =  F.binary_cross_entropy(hypothesis,y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() #혹시라도 optimizer가 grad를 구한게 있으면 0로 초기화 한다\n",
    "    cost.backward() # backpropagation을 수행\n",
    "    optimizer.step() # cost값을 minimize 하는 방법으로 수행합니다.\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch,nb_epochs,cost.item()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.0737],\n",
       "        [-3.8741],\n",
       "        [-0.4423],\n",
       "        [ 3.2403],\n",
       "        [ 5.9475],\n",
       "        [-2.3517]], requires_grad=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation 성능평가!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0012],\n",
      "        [0.9991],\n",
      "        [0.9988]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W)+b)\n",
    "print(hypothesis[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자 그럼 위의 값들을 가지고 0 또는1 찍어야 한다.\n",
    "\n",
    "We can change hypothesis (real number from 0 to 1) to binary prediction (either 0 or 1) by comparing them to 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ny_train = torch.FloatTensor([  \\n[0],  \\n[1],  \\n[1],  \\n[0],  \\n[1]  \\n])\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "print(prediction)\n",
    "\n",
    "\"\"\"\n",
    "y_train = torch.FloatTensor([  \n",
    "[0],  \n",
    "[1],  \n",
    "[1],  \n",
    "[0],  \n",
    "[1]  \n",
    "])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로 잘 구해진것을 볼 수 있습니다.그러나 자료(sample)이 엄청 많다면 이렇게 하나하나 비교하는게 힘들수 있습니다,  \n",
    "   \n",
    "그래서 우리는 ByteTensor값인 prediction을 .float()으로 변환하여 정답과 직접 비교 해보도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True]])\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = prediction.float() == y_train\n",
    "print(correct_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher Implementataion with Class\n",
    "\n",
    "위의 방법도 좋지만 실전에선 아래와 같은방법으로 데이터 학습을 할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module) : ##nn.Module이라는 추상클래스를 상속 받습니다.\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(6,1) # W,b가 들어있는 linear layer를 선언\n",
    "                                     # 위와같이 선언하면 W는 (8,1) 크기 b (1,) 를 나타낸다.\n",
    "                                     # 이렇게만 해줘도 d의 차원은 알 수 있다 d=8 , m은 ?? 아직,,\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x) :            # 아래 x가 self.linear(x)를 통과하고 나서 self.sigmoid에 넣어주면 이것이 전체 foward를 의미합니다.\n",
    "        return self.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.586920\n",
      "Epoch  100/1000 Cost: 5.526752\n",
      "Epoch  200/1000 Cost: 5.526547\n",
      "Epoch  300/1000 Cost: 5.526697\n",
      "Epoch  400/1000 Cost: 5.532208\n",
      "Epoch  500/1000 Cost: 5.528711\n",
      "Epoch  600/1000 Cost: 0.003727\n",
      "Epoch  700/1000 Cost: 0.001326\n",
      "Epoch  800/1000 Cost: 0.001249\n",
      "Epoch  900/1000 Cost: 0.001199\n"
     ]
    }
   ],
   "source": [
    "# Training Data\n",
    "x_train = torch.FloatTensor([  \n",
    "[7,8,8,9,5,4],  \n",
    "[3,8,4,6,5,0],  \n",
    "[6,6,7,6,7,4],  \n",
    "[3,8,2,1,6,2],  \n",
    "[1,8,2,6,6,4],  \n",
    "])\n",
    "\n",
    "y_train = torch.FloatTensor([  \n",
    "[0],  \n",
    "[1],  \n",
    "[1],  \n",
    "[0],  \n",
    "[1]  \n",
    "])\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.3) \n",
    "\"\"\"\n",
    "위와 다르게 W,b가 없다!?\n",
    "\"\"\"\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs) :\n",
    "    hypothesis = model(x_train)\n",
    "    cost =  F.binary_cross_entropy(hypothesis,y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() #혹시라도 optimizer가 grad를 구한게 있으면 0로 초기화 한다\n",
    "    cost.backward() # backpropagation을 수행\n",
    "    optimizer.step() # cost값을 minimize 하는 방법으로 수행합니다.\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch,nb_epochs,cost.item()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True]])\n"
     ]
    }
   ],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W)+b)\n",
    "prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "correct_prediction = prediction.float() == y_train\n",
    "print(correct_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
