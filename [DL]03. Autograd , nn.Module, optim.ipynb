{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch : Tensor\n",
    "\n",
    "Numpy는 GPU를 사용하여 수치 연산을 할 수가 없습니다. 그렇기 때문에 속도가 중유한 Deeplearning 에선 Numpy를 사용하기는 힘듭니다.  \n",
    "\n",
    "이번에는 Pytorch의 기본적인 개념인 Tensor에 대해 알아보겠습니다.  \n",
    "Pytorch Tensor는 기본적으로 Numpy 배열과 동일합니다. : Tensor는 N차원 배열이며, Pytorch Tensor는 연산을 위한 함수들을 제공합니다.  \n",
    "Numpy 배열과 같이 Pytorch Tensor는 딥러닝이나 연산 그래프, 변화도는 알지못하고, 과학적 분야의 연산을 이한 포괄적 도구 입니니다.\n",
    "\n",
    "그러나 Numpy와는 달리 Pytorch Tensor는 GPU를 활용하여 연산을 가속화할 수 있습니다.  \n",
    "GPU에서 Pytorch Tensor를 실행하기 위해서는 단지 새로운 자료형으로 Casting 해주기만 하면 됩니다.  \n",
    "\n",
    "여기에서는 Pytorch Tensor를 이용하여 2-계층의 신경망이 무작위 데이터를 맞추는 예제를 알아보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "dtype = torch.cuda.FloatTensor # GPU에서 실행하려면 이 주석을 제거하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 Tensor는 CPU Tensor GPU Tensor가 따로 존재 합니다. 서로의 연산은 불가합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉 계층의 차원이며, D_out은 출력 차원입니다:\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 무작위의 입력과 출력 데이터를 생성합니다.\n",
    "x = torch.randn(N, D_in).type(dtype)\n",
    "y = torch.randn(N, D_out).type(dtype)\n",
    "\n",
    "# 무작위로 가중치를 초기화합니다.\n",
    "w1 = torch.randn(D_in, H).type(dtype)\n",
    "w2 = torch.randn(H, D_out).type(dtype)\n",
    "\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(34562896., device='cuda:0')\n",
      "1 tensor(32339172., device='cuda:0')\n",
      "2 tensor(33135676., device='cuda:0')\n",
      "3 tensor(31322528., device='cuda:0')\n",
      "4 tensor(24819392., device='cuda:0')\n",
      "5 tensor(15951545., device='cuda:0')\n",
      "6 tensor(8803535., device='cuda:0')\n",
      "7 tensor(4619951.5000, device='cuda:0')\n",
      "8 tensor(2569621.5000, device='cuda:0')\n",
      "9 tensor(1604507.5000, device='cuda:0')\n",
      "10 tensor(1126430., device='cuda:0')\n",
      "11 tensor(861136.7500, device='cuda:0')\n",
      "12 tensor(693623.2500, device='cuda:0')\n",
      "13 tensor(575766.4375, device='cuda:0')\n",
      "14 tensor(486400.3750, device='cuda:0')\n",
      "15 tensor(415509.4375, device='cuda:0')\n",
      "16 tensor(357708.4062, device='cuda:0')\n",
      "17 tensor(309767.0938, device='cuda:0')\n",
      "18 tensor(269534.6250, device='cuda:0')\n",
      "19 tensor(235641.6406, device='cuda:0')\n",
      "20 tensor(206788.9375, device='cuda:0')\n",
      "21 tensor(182117., device='cuda:0')\n",
      "22 tensor(160865.4844, device='cuda:0')\n",
      "23 tensor(142508.7188, device='cuda:0')\n",
      "24 tensor(126587.3750, device='cuda:0')\n",
      "25 tensor(112740.2969, device='cuda:0')\n",
      "26 tensor(100643.4844, device='cuda:0')\n",
      "27 tensor(90038.5312, device='cuda:0')\n",
      "28 tensor(80706.5703, device='cuda:0')\n",
      "29 tensor(72494.3984, device='cuda:0')\n",
      "30 tensor(65245.4492, device='cuda:0')\n",
      "31 tensor(58832.1172, device='cuda:0')\n",
      "32 tensor(53134.7734, device='cuda:0')\n",
      "33 tensor(48071.8203, device='cuda:0')\n",
      "34 tensor(43552.9727, device='cuda:0')\n",
      "35 tensor(39517.8086, device='cuda:0')\n",
      "36 tensor(35908.0625, device='cuda:0')\n",
      "37 tensor(32670.9102, device='cuda:0')\n",
      "38 tensor(29760.8594, device='cuda:0')\n",
      "39 tensor(27144.5449, device='cuda:0')\n",
      "40 tensor(24790.9434, device='cuda:0')\n",
      "41 tensor(22666.8359, device='cuda:0')\n",
      "42 tensor(20746.2285, device='cuda:0')\n",
      "43 tensor(19007.9668, device='cuda:0')\n",
      "44 tensor(17431.5293, device='cuda:0')\n",
      "45 tensor(16001.6953, device='cuda:0')\n",
      "46 tensor(14704.2949, device='cuda:0')\n",
      "47 tensor(13524.3184, device='cuda:0')\n",
      "48 tensor(12451.6484, device='cuda:0')\n",
      "49 tensor(11472.7168, device='cuda:0')\n",
      "50 tensor(10579.2285, device='cuda:0')\n",
      "51 tensor(9762.4414, device='cuda:0')\n",
      "52 tensor(9014.7891, device='cuda:0')\n",
      "53 tensor(8330.3818, device='cuda:0')\n",
      "54 tensor(7702.8477, device='cuda:0')\n",
      "55 tensor(7126.8960, device='cuda:0')\n",
      "56 tensor(6598.1021, device='cuda:0')\n",
      "57 tensor(6112.1235, device='cuda:0')\n",
      "58 tensor(5665.0410, device='cuda:0')\n",
      "59 tensor(5253.5850, device='cuda:0')\n",
      "60 tensor(4874.8760, device='cuda:0')\n",
      "61 tensor(4525.6079, device='cuda:0')\n",
      "62 tensor(4203.4326, device='cuda:0')\n",
      "63 tensor(3906.0498, device='cuda:0')\n",
      "64 tensor(3631.6426, device='cuda:0')\n",
      "65 tensor(3377.9724, device='cuda:0')\n",
      "66 tensor(3143.3977, device='cuda:0')\n",
      "67 tensor(2926.4663, device='cuda:0')\n",
      "68 tensor(2725.5852, device='cuda:0')\n",
      "69 tensor(2539.5259, device='cuda:0')\n",
      "70 tensor(2367.0029, device='cuda:0')\n",
      "71 tensor(2207.0869, device='cuda:0')\n",
      "72 tensor(2058.7461, device='cuda:0')\n",
      "73 tensor(1921.1409, device='cuda:0')\n",
      "74 tensor(1793.3406, device='cuda:0')\n",
      "75 tensor(1674.5386, device='cuda:0')\n",
      "76 tensor(1564.1531, device='cuda:0')\n",
      "77 tensor(1461.5410, device='cuda:0')\n",
      "78 tensor(1366.0679, device='cuda:0')\n",
      "79 tensor(1277.2108, device='cuda:0')\n",
      "80 tensor(1194.5073, device='cuda:0')\n",
      "81 tensor(1117.5341, device='cuda:0')\n",
      "82 tensor(1045.8031, device='cuda:0')\n",
      "83 tensor(978.9209, device='cuda:0')\n",
      "84 tensor(916.5754, device='cuda:0')\n",
      "85 tensor(858.4243, device='cuda:0')\n",
      "86 tensor(804.1923, device='cuda:0')\n",
      "87 tensor(753.5975, device='cuda:0')\n",
      "88 tensor(706.3722, device='cuda:0')\n",
      "89 tensor(662.2782, device='cuda:0')\n",
      "90 tensor(621.0431, device='cuda:0')\n",
      "91 tensor(582.5165, device='cuda:0')\n",
      "92 tensor(546.5021, device='cuda:0')\n",
      "93 tensor(512.8290, device='cuda:0')\n",
      "94 tensor(481.3388, device='cuda:0')\n",
      "95 tensor(451.8815, device='cuda:0')\n",
      "96 tensor(424.3199, device='cuda:0')\n",
      "97 tensor(398.5146, device='cuda:0')\n",
      "98 tensor(374.3537, device='cuda:0')\n",
      "99 tensor(351.7259, device='cuda:0')\n",
      "100 tensor(330.5268, device='cuda:0')\n",
      "101 tensor(310.6634, device='cuda:0')\n",
      "102 tensor(292.0528, device='cuda:0')\n",
      "103 tensor(274.6038, device='cuda:0')\n",
      "104 tensor(258.2469, device='cuda:0')\n",
      "105 tensor(242.8971, device='cuda:0')\n",
      "106 tensor(228.5187, device='cuda:0')\n",
      "107 tensor(215.0354, device='cuda:0')\n",
      "108 tensor(202.3848, device='cuda:0')\n",
      "109 tensor(190.5116, device='cuda:0')\n",
      "110 tensor(179.3617, device='cuda:0')\n",
      "111 tensor(168.8935, device='cuda:0')\n",
      "112 tensor(159.0618, device='cuda:0')\n",
      "113 tensor(149.8225, device='cuda:0')\n",
      "114 tensor(141.1462, device='cuda:0')\n",
      "115 tensor(132.9914, device='cuda:0')\n",
      "116 tensor(125.3208, device='cuda:0')\n",
      "117 tensor(118.1087, device='cuda:0')\n",
      "118 tensor(111.3287, device='cuda:0')\n",
      "119 tensor(104.9543, device='cuda:0')\n",
      "120 tensor(98.9576, device='cuda:0')\n",
      "121 tensor(93.3167, device='cuda:0')\n",
      "122 tensor(88.0057, device='cuda:0')\n",
      "123 tensor(83.0082, device='cuda:0')\n",
      "124 tensor(78.3074, device='cuda:0')\n",
      "125 tensor(73.8808, device='cuda:0')\n",
      "126 tensor(69.7135, device='cuda:0')\n",
      "127 tensor(65.7887, device='cuda:0')\n",
      "128 tensor(62.0913, device='cuda:0')\n",
      "129 tensor(58.6093, device='cuda:0')\n",
      "130 tensor(55.3311, device='cuda:0')\n",
      "131 tensor(52.2405, device='cuda:0')\n",
      "132 tensor(49.3270, device='cuda:0')\n",
      "133 tensor(46.5817, device='cuda:0')\n",
      "134 tensor(43.9957, device='cuda:0')\n",
      "135 tensor(41.5578, device='cuda:0')\n",
      "136 tensor(39.2579, device='cuda:0')\n",
      "137 tensor(37.0904, device='cuda:0')\n",
      "138 tensor(35.0452, device='cuda:0')\n",
      "139 tensor(33.1152, device='cuda:0')\n",
      "140 tensor(31.2960, device='cuda:0')\n",
      "141 tensor(29.5798, device='cuda:0')\n",
      "142 tensor(27.9614, device='cuda:0')\n",
      "143 tensor(26.4341, device='cuda:0')\n",
      "144 tensor(24.9920, device='cuda:0')\n",
      "145 tensor(23.6304, device='cuda:0')\n",
      "146 tensor(22.3464, device='cuda:0')\n",
      "147 tensor(21.1340, device='cuda:0')\n",
      "148 tensor(19.9881, device='cuda:0')\n",
      "149 tensor(18.9069, device='cuda:0')\n",
      "150 tensor(17.8857, device='cuda:0')\n",
      "151 tensor(16.9210, device='cuda:0')\n",
      "152 tensor(16.0106, device='cuda:0')\n",
      "153 tensor(15.1497, device='cuda:0')\n",
      "154 tensor(14.3366, device='cuda:0')\n",
      "155 tensor(13.5685, device='cuda:0')\n",
      "156 tensor(12.8423, device='cuda:0')\n",
      "157 tensor(12.1567, device='cuda:0')\n",
      "158 tensor(11.5080, device='cuda:0')\n",
      "159 tensor(10.8951, device='cuda:0')\n",
      "160 tensor(10.3158, device='cuda:0')\n",
      "161 tensor(9.7682, device='cuda:0')\n",
      "162 tensor(9.2505, device='cuda:0')\n",
      "163 tensor(8.7611, device='cuda:0')\n",
      "164 tensor(8.2979, device='cuda:0')\n",
      "165 tensor(7.8602, device='cuda:0')\n",
      "166 tensor(7.4457, device='cuda:0')\n",
      "167 tensor(7.0539, device='cuda:0')\n",
      "168 tensor(6.6832, device='cuda:0')\n",
      "169 tensor(6.3328, device='cuda:0')\n",
      "170 tensor(6.0019, device='cuda:0')\n",
      "171 tensor(5.6889, device='cuda:0')\n",
      "172 tensor(5.3927, device='cuda:0')\n",
      "173 tensor(5.1119, device='cuda:0')\n",
      "174 tensor(4.8466, device='cuda:0')\n",
      "175 tensor(4.5951, device='cuda:0')\n",
      "176 tensor(4.3571, device='cuda:0')\n",
      "177 tensor(4.1319, device='cuda:0')\n",
      "178 tensor(3.9187, device='cuda:0')\n",
      "179 tensor(3.7164, device='cuda:0')\n",
      "180 tensor(3.5252, device='cuda:0')\n",
      "181 tensor(3.3439, device='cuda:0')\n",
      "182 tensor(3.1721, device='cuda:0')\n",
      "183 tensor(3.0094, device='cuda:0')\n",
      "184 tensor(2.8553, device='cuda:0')\n",
      "185 tensor(2.7095, device='cuda:0')\n",
      "186 tensor(2.5712, device='cuda:0')\n",
      "187 tensor(2.4400, device='cuda:0')\n",
      "188 tensor(2.3157, device='cuda:0')\n",
      "189 tensor(2.1979, device='cuda:0')\n",
      "190 tensor(2.0864, device='cuda:0')\n",
      "191 tensor(1.9803, device='cuda:0')\n",
      "192 tensor(1.8800, device='cuda:0')\n",
      "193 tensor(1.7852, device='cuda:0')\n",
      "194 tensor(1.6949, device='cuda:0')\n",
      "195 tensor(1.6095, device='cuda:0')\n",
      "196 tensor(1.5284, device='cuda:0')\n",
      "197 tensor(1.4514, device='cuda:0')\n",
      "198 tensor(1.3784, device='cuda:0')\n",
      "199 tensor(1.3093, device='cuda:0')\n",
      "200 tensor(1.2436, device='cuda:0')\n",
      "201 tensor(1.1815, device='cuda:0')\n",
      "202 tensor(1.1224, device='cuda:0')\n",
      "203 tensor(1.0663, device='cuda:0')\n",
      "204 tensor(1.0132, device='cuda:0')\n",
      "205 tensor(0.9627, device='cuda:0')\n",
      "206 tensor(0.9147, device='cuda:0')\n",
      "207 tensor(0.8694, device='cuda:0')\n",
      "208 tensor(0.8261, device='cuda:0')\n",
      "209 tensor(0.7852, device='cuda:0')\n",
      "210 tensor(0.7464, device='cuda:0')\n",
      "211 tensor(0.7095, device='cuda:0')\n",
      "212 tensor(0.6744, device='cuda:0')\n",
      "213 tensor(0.6411, device='cuda:0')\n",
      "214 tensor(0.6095, device='cuda:0')\n",
      "215 tensor(0.5796, device='cuda:0')\n",
      "216 tensor(0.5511, device='cuda:0')\n",
      "217 tensor(0.5241, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 tensor(0.4984, device='cuda:0')\n",
      "219 tensor(0.4739, device='cuda:0')\n",
      "220 tensor(0.4508, device='cuda:0')\n",
      "221 tensor(0.4287, device='cuda:0')\n",
      "222 tensor(0.4078, device='cuda:0')\n",
      "223 tensor(0.3879, device='cuda:0')\n",
      "224 tensor(0.3690, device='cuda:0')\n",
      "225 tensor(0.3511, device='cuda:0')\n",
      "226 tensor(0.3340, device='cuda:0')\n",
      "227 tensor(0.3178, device='cuda:0')\n",
      "228 tensor(0.3024, device='cuda:0')\n",
      "229 tensor(0.2878, device='cuda:0')\n",
      "230 tensor(0.2739, device='cuda:0')\n",
      "231 tensor(0.2606, device='cuda:0')\n",
      "232 tensor(0.2480, device='cuda:0')\n",
      "233 tensor(0.2360, device='cuda:0')\n",
      "234 tensor(0.2247, device='cuda:0')\n",
      "235 tensor(0.2139, device='cuda:0')\n",
      "236 tensor(0.2036, device='cuda:0')\n",
      "237 tensor(0.1938, device='cuda:0')\n",
      "238 tensor(0.1845, device='cuda:0')\n",
      "239 tensor(0.1757, device='cuda:0')\n",
      "240 tensor(0.1673, device='cuda:0')\n",
      "241 tensor(0.1593, device='cuda:0')\n",
      "242 tensor(0.1517, device='cuda:0')\n",
      "243 tensor(0.1444, device='cuda:0')\n",
      "244 tensor(0.1375, device='cuda:0')\n",
      "245 tensor(0.1310, device='cuda:0')\n",
      "246 tensor(0.1248, device='cuda:0')\n",
      "247 tensor(0.1188, device='cuda:0')\n",
      "248 tensor(0.1132, device='cuda:0')\n",
      "249 tensor(0.1078, device='cuda:0')\n",
      "250 tensor(0.1027, device='cuda:0')\n",
      "251 tensor(0.0979, device='cuda:0')\n",
      "252 tensor(0.0932, device='cuda:0')\n",
      "253 tensor(0.0888, device='cuda:0')\n",
      "254 tensor(0.0847, device='cuda:0')\n",
      "255 tensor(0.0807, device='cuda:0')\n",
      "256 tensor(0.0769, device='cuda:0')\n",
      "257 tensor(0.0732, device='cuda:0')\n",
      "258 tensor(0.0698, device='cuda:0')\n",
      "259 tensor(0.0665, device='cuda:0')\n",
      "260 tensor(0.0634, device='cuda:0')\n",
      "261 tensor(0.0604, device='cuda:0')\n",
      "262 tensor(0.0576, device='cuda:0')\n",
      "263 tensor(0.0549, device='cuda:0')\n",
      "264 tensor(0.0523, device='cuda:0')\n",
      "265 tensor(0.0499, device='cuda:0')\n",
      "266 tensor(0.0475, device='cuda:0')\n",
      "267 tensor(0.0453, device='cuda:0')\n",
      "268 tensor(0.0432, device='cuda:0')\n",
      "269 tensor(0.0412, device='cuda:0')\n",
      "270 tensor(0.0393, device='cuda:0')\n",
      "271 tensor(0.0375, device='cuda:0')\n",
      "272 tensor(0.0358, device='cuda:0')\n",
      "273 tensor(0.0341, device='cuda:0')\n",
      "274 tensor(0.0325, device='cuda:0')\n",
      "275 tensor(0.0310, device='cuda:0')\n",
      "276 tensor(0.0296, device='cuda:0')\n",
      "277 tensor(0.0282, device='cuda:0')\n",
      "278 tensor(0.0269, device='cuda:0')\n",
      "279 tensor(0.0257, device='cuda:0')\n",
      "280 tensor(0.0245, device='cuda:0')\n",
      "281 tensor(0.0234, device='cuda:0')\n",
      "282 tensor(0.0223, device='cuda:0')\n",
      "283 tensor(0.0213, device='cuda:0')\n",
      "284 tensor(0.0203, device='cuda:0')\n",
      "285 tensor(0.0194, device='cuda:0')\n",
      "286 tensor(0.0185, device='cuda:0')\n",
      "287 tensor(0.0177, device='cuda:0')\n",
      "288 tensor(0.0169, device='cuda:0')\n",
      "289 tensor(0.0161, device='cuda:0')\n",
      "290 tensor(0.0154, device='cuda:0')\n",
      "291 tensor(0.0147, device='cuda:0')\n",
      "292 tensor(0.0140, device='cuda:0')\n",
      "293 tensor(0.0134, device='cuda:0')\n",
      "294 tensor(0.0128, device='cuda:0')\n",
      "295 tensor(0.0122, device='cuda:0')\n",
      "296 tensor(0.0117, device='cuda:0')\n",
      "297 tensor(0.0111, device='cuda:0')\n",
      "298 tensor(0.0107, device='cuda:0')\n",
      "299 tensor(0.0102, device='cuda:0')\n",
      "300 tensor(0.0097, device='cuda:0')\n",
      "301 tensor(0.0093, device='cuda:0')\n",
      "302 tensor(0.0089, device='cuda:0')\n",
      "303 tensor(0.0085, device='cuda:0')\n",
      "304 tensor(0.0081, device='cuda:0')\n",
      "305 tensor(0.0078, device='cuda:0')\n",
      "306 tensor(0.0074, device='cuda:0')\n",
      "307 tensor(0.0071, device='cuda:0')\n",
      "308 tensor(0.0068, device='cuda:0')\n",
      "309 tensor(0.0065, device='cuda:0')\n",
      "310 tensor(0.0062, device='cuda:0')\n",
      "311 tensor(0.0059, device='cuda:0')\n",
      "312 tensor(0.0057, device='cuda:0')\n",
      "313 tensor(0.0054, device='cuda:0')\n",
      "314 tensor(0.0052, device='cuda:0')\n",
      "315 tensor(0.0050, device='cuda:0')\n",
      "316 tensor(0.0048, device='cuda:0')\n",
      "317 tensor(0.0046, device='cuda:0')\n",
      "318 tensor(0.0044, device='cuda:0')\n",
      "319 tensor(0.0042, device='cuda:0')\n",
      "320 tensor(0.0040, device='cuda:0')\n",
      "321 tensor(0.0039, device='cuda:0')\n",
      "322 tensor(0.0037, device='cuda:0')\n",
      "323 tensor(0.0035, device='cuda:0')\n",
      "324 tensor(0.0034, device='cuda:0')\n",
      "325 tensor(0.0033, device='cuda:0')\n",
      "326 tensor(0.0031, device='cuda:0')\n",
      "327 tensor(0.0030, device='cuda:0')\n",
      "328 tensor(0.0029, device='cuda:0')\n",
      "329 tensor(0.0028, device='cuda:0')\n",
      "330 tensor(0.0027, device='cuda:0')\n",
      "331 tensor(0.0025, device='cuda:0')\n",
      "332 tensor(0.0024, device='cuda:0')\n",
      "333 tensor(0.0023, device='cuda:0')\n",
      "334 tensor(0.0023, device='cuda:0')\n",
      "335 tensor(0.0022, device='cuda:0')\n",
      "336 tensor(0.0021, device='cuda:0')\n",
      "337 tensor(0.0020, device='cuda:0')\n",
      "338 tensor(0.0019, device='cuda:0')\n",
      "339 tensor(0.0019, device='cuda:0')\n",
      "340 tensor(0.0018, device='cuda:0')\n",
      "341 tensor(0.0017, device='cuda:0')\n",
      "342 tensor(0.0017, device='cuda:0')\n",
      "343 tensor(0.0016, device='cuda:0')\n",
      "344 tensor(0.0015, device='cuda:0')\n",
      "345 tensor(0.0015, device='cuda:0')\n",
      "346 tensor(0.0014, device='cuda:0')\n",
      "347 tensor(0.0014, device='cuda:0')\n",
      "348 tensor(0.0013, device='cuda:0')\n",
      "349 tensor(0.0013, device='cuda:0')\n",
      "350 tensor(0.0012, device='cuda:0')\n",
      "351 tensor(0.0012, device='cuda:0')\n",
      "352 tensor(0.0011, device='cuda:0')\n",
      "353 tensor(0.0011, device='cuda:0')\n",
      "354 tensor(0.0011, device='cuda:0')\n",
      "355 tensor(0.0010, device='cuda:0')\n",
      "356 tensor(0.0010, device='cuda:0')\n",
      "357 tensor(0.0010, device='cuda:0')\n",
      "358 tensor(0.0009, device='cuda:0')\n",
      "359 tensor(0.0009, device='cuda:0')\n",
      "360 tensor(0.0009, device='cuda:0')\n",
      "361 tensor(0.0008, device='cuda:0')\n",
      "362 tensor(0.0008, device='cuda:0')\n",
      "363 tensor(0.0008, device='cuda:0')\n",
      "364 tensor(0.0008, device='cuda:0')\n",
      "365 tensor(0.0007, device='cuda:0')\n",
      "366 tensor(0.0007, device='cuda:0')\n",
      "367 tensor(0.0007, device='cuda:0')\n",
      "368 tensor(0.0007, device='cuda:0')\n",
      "369 tensor(0.0007, device='cuda:0')\n",
      "370 tensor(0.0006, device='cuda:0')\n",
      "371 tensor(0.0006, device='cuda:0')\n",
      "372 tensor(0.0006, device='cuda:0')\n",
      "373 tensor(0.0006, device='cuda:0')\n",
      "374 tensor(0.0006, device='cuda:0')\n",
      "375 tensor(0.0005, device='cuda:0')\n",
      "376 tensor(0.0005, device='cuda:0')\n",
      "377 tensor(0.0005, device='cuda:0')\n",
      "378 tensor(0.0005, device='cuda:0')\n",
      "379 tensor(0.0005, device='cuda:0')\n",
      "380 tensor(0.0005, device='cuda:0')\n",
      "381 tensor(0.0005, device='cuda:0')\n",
      "382 tensor(0.0004, device='cuda:0')\n",
      "383 tensor(0.0004, device='cuda:0')\n",
      "384 tensor(0.0004, device='cuda:0')\n",
      "385 tensor(0.0004, device='cuda:0')\n",
      "386 tensor(0.0004, device='cuda:0')\n",
      "387 tensor(0.0004, device='cuda:0')\n",
      "388 tensor(0.0004, device='cuda:0')\n",
      "389 tensor(0.0004, device='cuda:0')\n",
      "390 tensor(0.0004, device='cuda:0')\n",
      "391 tensor(0.0003, device='cuda:0')\n",
      "392 tensor(0.0003, device='cuda:0')\n",
      "393 tensor(0.0003, device='cuda:0')\n",
      "394 tensor(0.0003, device='cuda:0')\n",
      "395 tensor(0.0003, device='cuda:0')\n",
      "396 tensor(0.0003, device='cuda:0')\n",
      "397 tensor(0.0003, device='cuda:0')\n",
      "398 tensor(0.0003, device='cuda:0')\n",
      "399 tensor(0.0003, device='cuda:0')\n",
      "400 tensor(0.0003, device='cuda:0')\n",
      "401 tensor(0.0003, device='cuda:0')\n",
      "402 tensor(0.0003, device='cuda:0')\n",
      "403 tensor(0.0003, device='cuda:0')\n",
      "404 tensor(0.0003, device='cuda:0')\n",
      "405 tensor(0.0002, device='cuda:0')\n",
      "406 tensor(0.0002, device='cuda:0')\n",
      "407 tensor(0.0002, device='cuda:0')\n",
      "408 tensor(0.0002, device='cuda:0')\n",
      "409 tensor(0.0002, device='cuda:0')\n",
      "410 tensor(0.0002, device='cuda:0')\n",
      "411 tensor(0.0002, device='cuda:0')\n",
      "412 tensor(0.0002, device='cuda:0')\n",
      "413 tensor(0.0002, device='cuda:0')\n",
      "414 tensor(0.0002, device='cuda:0')\n",
      "415 tensor(0.0002, device='cuda:0')\n",
      "416 tensor(0.0002, device='cuda:0')\n",
      "417 tensor(0.0002, device='cuda:0')\n",
      "418 tensor(0.0002, device='cuda:0')\n",
      "419 tensor(0.0002, device='cuda:0')\n",
      "420 tensor(0.0002, device='cuda:0')\n",
      "421 tensor(0.0002, device='cuda:0')\n",
      "422 tensor(0.0002, device='cuda:0')\n",
      "423 tensor(0.0002, device='cuda:0')\n",
      "424 tensor(0.0002, device='cuda:0')\n",
      "425 tensor(0.0002, device='cuda:0')\n",
      "426 tensor(0.0002, device='cuda:0')\n",
      "427 tensor(0.0002, device='cuda:0')\n",
      "428 tensor(0.0001, device='cuda:0')\n",
      "429 tensor(0.0001, device='cuda:0')\n",
      "430 tensor(0.0001, device='cuda:0')\n",
      "431 tensor(0.0001, device='cuda:0')\n",
      "432 tensor(0.0001, device='cuda:0')\n",
      "433 tensor(0.0001, device='cuda:0')\n",
      "434 tensor(0.0001, device='cuda:0')\n",
      "435 tensor(0.0001, device='cuda:0')\n",
      "436 tensor(0.0001, device='cuda:0')\n",
      "437 tensor(0.0001, device='cuda:0')\n",
      "438 tensor(0.0001, device='cuda:0')\n",
      "439 tensor(0.0001, device='cuda:0')\n",
      "440 tensor(0.0001, device='cuda:0')\n",
      "441 tensor(0.0001, device='cuda:0')\n",
      "442 tensor(0.0001, device='cuda:0')\n",
      "443 tensor(0.0001, device='cuda:0')\n",
      "444 tensor(0.0001, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445 tensor(0.0001, device='cuda:0')\n",
      "446 tensor(0.0001, device='cuda:0')\n",
      "447 tensor(0.0001, device='cuda:0')\n",
      "448 tensor(0.0001, device='cuda:0')\n",
      "449 tensor(0.0001, device='cuda:0')\n",
      "450 tensor(0.0001, device='cuda:0')\n",
      "451 tensor(9.8954e-05, device='cuda:0')\n",
      "452 tensor(9.7234e-05, device='cuda:0')\n",
      "453 tensor(9.5974e-05, device='cuda:0')\n",
      "454 tensor(9.4319e-05, device='cuda:0')\n",
      "455 tensor(9.2771e-05, device='cuda:0')\n",
      "456 tensor(9.1190e-05, device='cuda:0')\n",
      "457 tensor(8.9928e-05, device='cuda:0')\n",
      "458 tensor(8.8430e-05, device='cuda:0')\n",
      "459 tensor(8.7281e-05, device='cuda:0')\n",
      "460 tensor(8.6014e-05, device='cuda:0')\n",
      "461 tensor(8.4562e-05, device='cuda:0')\n",
      "462 tensor(8.3350e-05, device='cuda:0')\n",
      "463 tensor(8.1975e-05, device='cuda:0')\n",
      "464 tensor(8.0821e-05, device='cuda:0')\n",
      "465 tensor(7.9701e-05, device='cuda:0')\n",
      "466 tensor(7.8050e-05, device='cuda:0')\n",
      "467 tensor(7.7056e-05, device='cuda:0')\n",
      "468 tensor(7.5922e-05, device='cuda:0')\n",
      "469 tensor(7.4874e-05, device='cuda:0')\n",
      "470 tensor(7.3708e-05, device='cuda:0')\n",
      "471 tensor(7.2556e-05, device='cuda:0')\n",
      "472 tensor(7.1535e-05, device='cuda:0')\n",
      "473 tensor(7.0721e-05, device='cuda:0')\n",
      "474 tensor(6.9633e-05, device='cuda:0')\n",
      "475 tensor(6.8581e-05, device='cuda:0')\n",
      "476 tensor(6.7887e-05, device='cuda:0')\n",
      "477 tensor(6.7038e-05, device='cuda:0')\n",
      "478 tensor(6.6157e-05, device='cuda:0')\n",
      "479 tensor(6.5277e-05, device='cuda:0')\n",
      "480 tensor(6.4548e-05, device='cuda:0')\n",
      "481 tensor(6.3761e-05, device='cuda:0')\n",
      "482 tensor(6.2754e-05, device='cuda:0')\n",
      "483 tensor(6.1899e-05, device='cuda:0')\n",
      "484 tensor(6.1026e-05, device='cuda:0')\n",
      "485 tensor(6.0215e-05, device='cuda:0')\n",
      "486 tensor(5.9485e-05, device='cuda:0')\n",
      "487 tensor(5.8952e-05, device='cuda:0')\n",
      "488 tensor(5.8247e-05, device='cuda:0')\n",
      "489 tensor(5.7408e-05, device='cuda:0')\n",
      "490 tensor(5.6594e-05, device='cuda:0')\n",
      "491 tensor(5.5962e-05, device='cuda:0')\n",
      "492 tensor(5.5516e-05, device='cuda:0')\n",
      "493 tensor(5.4661e-05, device='cuda:0')\n",
      "494 tensor(5.4125e-05, device='cuda:0')\n",
      "495 tensor(5.3590e-05, device='cuda:0')\n",
      "496 tensor(5.2847e-05, device='cuda:0')\n",
      "497 tensor(5.2216e-05, device='cuda:0')\n",
      "498 tensor(5.1530e-05, device='cuda:0')\n",
      "499 tensor(5.0765e-05, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다.\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # 손실(loss)을 계산하고 출력합니다.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # 경사하강법(Gradient Descent)를 사용하여 가중치를 갱신합니다.\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "\n",
    "### Pytorch : Variables 과 autograd\n",
    "\n",
    "위의 예제에서 우리는 신경망의 순전파 단계와 역전파 단계를 수동으로 구현하였습니다. 작은 2-계층 신경망에서 역전파 단계를 직접 구현하는것은 쉬운일입니다. 하지만 대규모의복잡한 신경망에서는 어려운일일 수도 있습니다.  \n",
    "\n",
    "다행이도 자동 미분을 사용하면 신경망에서 역전파 단계의 연산의 자동화가 가능합니다. Pytorch의 autograd패키지는 이기능을 정확히 제공합니다. Autograd를 사용할 때, 신경망의 순전파 단계는 연산 그래프를 정의합니다.  그래프의 노드(node)는 Tensor이며, Edge는 입력 Tensor로부터 출력 Tensor를 만들어내는 함수입니다. 이 그래프를 통해 역전파를 하게 되면 변화도를 쉽게 계산할 수 있습니다.  \n",
    "\n",
    "이 는 복잡해 보이지만 실제로 사용하는것은 간단합니다. Pytorch Tensor를 Variable 객체로 감싸게 되면 이 Variable 이 연산 그래프에서 노드로 represent됩니다.x가 Variable 일떄, x.data는 그 값을 갖는 Tensor이며 x.grad는 어떤 스칼라 값에 대해 x에 대한 변화도를 갖는 또 다른 Variable입니다.  \n",
    "\n",
    "PyTorch Variable 은 Pytorch Tensor 와 동일한 API를 제공합니다. Tensor에서 할 수 있는 (거의) 모든 연산은 Variable에서도 할 수 있습니다. 차이점은 연산 그래프를 정의할 떄 Variable을 사용하면, 자동으로 변화도를 계산할 수 있다는 것입니다.  \n",
    "\n",
    "여기에서는 Pytorch Variable과 autograd를 사용하여 2-계층 신경망을 구현합니다. 이제 더이상 신경망의 역전파 단계를 직접 구현할 필요가 없습니다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(37168900., grad_fn=<SumBackward0>)\n",
      "1\n",
      "tensor(30610004., grad_fn=<SumBackward0>)\n",
      "2\n",
      "tensor(28078846., grad_fn=<SumBackward0>)\n",
      "3\n",
      "tensor(24716638., grad_fn=<SumBackward0>)\n",
      "4\n",
      "tensor(19395418., grad_fn=<SumBackward0>)\n",
      "5\n",
      "tensor(13310531., grad_fn=<SumBackward0>)\n",
      "6\n",
      "tensor(8276153.5000, grad_fn=<SumBackward0>)\n",
      "7\n",
      "tensor(4943476.5000, grad_fn=<SumBackward0>)\n",
      "8\n",
      "tensor(3016792.7500, grad_fn=<SumBackward0>)\n",
      "9\n",
      "tensor(1957507.2500, grad_fn=<SumBackward0>)\n",
      "10\n",
      "tensor(1367625.3750, grad_fn=<SumBackward0>)\n",
      "11\n",
      "tensor(1020840.5625, grad_fn=<SumBackward0>)\n",
      "12\n",
      "tensor(801120.8125, grad_fn=<SumBackward0>)\n",
      "13\n",
      "tensor(650637.5625, grad_fn=<SumBackward0>)\n",
      "14\n",
      "tensor(540531.7500, grad_fn=<SumBackward0>)\n",
      "15\n",
      "tensor(455873.9375, grad_fn=<SumBackward0>)\n",
      "16\n",
      "tensor(388558.9375, grad_fn=<SumBackward0>)\n",
      "17\n",
      "tensor(333845.0938, grad_fn=<SumBackward0>)\n",
      "18\n",
      "tensor(288591., grad_fn=<SumBackward0>)\n",
      "19\n",
      "tensor(250741.7188, grad_fn=<SumBackward0>)\n",
      "20\n",
      "tensor(218881.1250, grad_fn=<SumBackward0>)\n",
      "21\n",
      "tensor(191858.1406, grad_fn=<SumBackward0>)\n",
      "22\n",
      "tensor(168804.7656, grad_fn=<SumBackward0>)\n",
      "23\n",
      "tensor(149034.5000, grad_fn=<SumBackward0>)\n",
      "24\n",
      "tensor(132012.6875, grad_fn=<SumBackward0>)\n",
      "25\n",
      "tensor(117299.1250, grad_fn=<SumBackward0>)\n",
      "26\n",
      "tensor(104547.5078, grad_fn=<SumBackward0>)\n",
      "27\n",
      "tensor(93441.5781, grad_fn=<SumBackward0>)\n",
      "28\n",
      "tensor(83736.4219, grad_fn=<SumBackward0>)\n",
      "29\n",
      "tensor(75228.3203, grad_fn=<SumBackward0>)\n",
      "30\n",
      "tensor(67748.1406, grad_fn=<SumBackward0>)\n",
      "31\n",
      "tensor(61153.2656, grad_fn=<SumBackward0>)\n",
      "32\n",
      "tensor(55331.9961, grad_fn=<SumBackward0>)\n",
      "33\n",
      "tensor(50164.8867, grad_fn=<SumBackward0>)\n",
      "34\n",
      "tensor(45567.7852, grad_fn=<SumBackward0>)\n",
      "35\n",
      "tensor(41466.1211, grad_fn=<SumBackward0>)\n",
      "36\n",
      "tensor(37798.6055, grad_fn=<SumBackward0>)\n",
      "37\n",
      "tensor(34512.0391, grad_fn=<SumBackward0>)\n",
      "38\n",
      "tensor(31560.4590, grad_fn=<SumBackward0>)\n",
      "39\n",
      "tensor(28904.9844, grad_fn=<SumBackward0>)\n",
      "40\n",
      "tensor(26510.7637, grad_fn=<SumBackward0>)\n",
      "41\n",
      "tensor(24348.4844, grad_fn=<SumBackward0>)\n",
      "42\n",
      "tensor(22394.4570, grad_fn=<SumBackward0>)\n",
      "43\n",
      "tensor(20625.1523, grad_fn=<SumBackward0>)\n",
      "44\n",
      "tensor(19021.1973, grad_fn=<SumBackward0>)\n",
      "45\n",
      "tensor(17566.2656, grad_fn=<SumBackward0>)\n",
      "46\n",
      "tensor(16240.6914, grad_fn=<SumBackward0>)\n",
      "47\n",
      "tensor(15030.4219, grad_fn=<SumBackward0>)\n",
      "48\n",
      "tensor(13925.2686, grad_fn=<SumBackward0>)\n",
      "49\n",
      "tensor(12913.1748, grad_fn=<SumBackward0>)\n",
      "50\n",
      "tensor(11985.9736, grad_fn=<SumBackward0>)\n",
      "51\n",
      "tensor(11135.3174, grad_fn=<SumBackward0>)\n",
      "52\n",
      "tensor(10354.2168, grad_fn=<SumBackward0>)\n",
      "53\n",
      "tensor(9635.7822, grad_fn=<SumBackward0>)\n",
      "54\n",
      "tensor(8974.0928, grad_fn=<SumBackward0>)\n",
      "55\n",
      "tensor(8364.0879, grad_fn=<SumBackward0>)\n",
      "56\n",
      "tensor(7800.9863, grad_fn=<SumBackward0>)\n",
      "57\n",
      "tensor(7279.7803, grad_fn=<SumBackward0>)\n",
      "58\n",
      "tensor(6798.0913, grad_fn=<SumBackward0>)\n",
      "59\n",
      "tensor(6352.2798, grad_fn=<SumBackward0>)\n",
      "60\n",
      "tensor(5939.3096, grad_fn=<SumBackward0>)\n",
      "61\n",
      "tensor(5556.4111, grad_fn=<SumBackward0>)\n",
      "62\n",
      "tensor(5201.1748, grad_fn=<SumBackward0>)\n",
      "63\n",
      "tensor(4871.3765, grad_fn=<SumBackward0>)\n",
      "64\n",
      "tensor(4564.9551, grad_fn=<SumBackward0>)\n",
      "65\n",
      "tensor(4279.9287, grad_fn=<SumBackward0>)\n",
      "66\n",
      "tensor(4014.7639, grad_fn=<SumBackward0>)\n",
      "67\n",
      "tensor(3767.7568, grad_fn=<SumBackward0>)\n",
      "68\n",
      "tensor(3537.5549, grad_fn=<SumBackward0>)\n",
      "69\n",
      "tensor(3322.9309, grad_fn=<SumBackward0>)\n",
      "70\n",
      "tensor(3122.7109, grad_fn=<SumBackward0>)\n",
      "71\n",
      "tensor(2935.7065, grad_fn=<SumBackward0>)\n",
      "72\n",
      "tensor(2761.1626, grad_fn=<SumBackward0>)\n",
      "73\n",
      "tensor(2598.0994, grad_fn=<SumBackward0>)\n",
      "74\n",
      "tensor(2445.6296, grad_fn=<SumBackward0>)\n",
      "75\n",
      "tensor(2302.9634, grad_fn=<SumBackward0>)\n",
      "76\n",
      "tensor(2169.3914, grad_fn=<SumBackward0>)\n",
      "77\n",
      "tensor(2044.2867, grad_fn=<SumBackward0>)\n",
      "78\n",
      "tensor(1927.0142, grad_fn=<SumBackward0>)\n",
      "79\n",
      "tensor(1817.0964, grad_fn=<SumBackward0>)\n",
      "80\n",
      "tensor(1713.9993, grad_fn=<SumBackward0>)\n",
      "81\n",
      "tensor(1617.2523, grad_fn=<SumBackward0>)\n",
      "82\n",
      "tensor(1526.4403, grad_fn=<SumBackward0>)\n",
      "83\n",
      "tensor(1441.1376, grad_fn=<SumBackward0>)\n",
      "84\n",
      "tensor(1360.9950, grad_fn=<SumBackward0>)\n",
      "85\n",
      "tensor(1285.6754, grad_fn=<SumBackward0>)\n",
      "86\n",
      "tensor(1214.8291, grad_fn=<SumBackward0>)\n",
      "87\n",
      "tensor(1148.1843, grad_fn=<SumBackward0>)\n",
      "88\n",
      "tensor(1085.4758, grad_fn=<SumBackward0>)\n",
      "89\n",
      "tensor(1026.4971, grad_fn=<SumBackward0>)\n",
      "90\n",
      "tensor(971.0681, grad_fn=<SumBackward0>)\n",
      "91\n",
      "tensor(918.8212, grad_fn=<SumBackward0>)\n",
      "92\n",
      "tensor(869.6104, grad_fn=<SumBackward0>)\n",
      "93\n",
      "tensor(823.2311, grad_fn=<SumBackward0>)\n",
      "94\n",
      "tensor(779.4720, grad_fn=<SumBackward0>)\n",
      "95\n",
      "tensor(738.1946, grad_fn=<SumBackward0>)\n",
      "96\n",
      "tensor(699.2451, grad_fn=<SumBackward0>)\n",
      "97\n",
      "tensor(662.4919, grad_fn=<SumBackward0>)\n",
      "98\n",
      "tensor(627.7987, grad_fn=<SumBackward0>)\n",
      "99\n",
      "tensor(595.0336, grad_fn=<SumBackward0>)\n",
      "100\n",
      "tensor(564.0846, grad_fn=<SumBackward0>)\n",
      "101\n",
      "tensor(534.8399, grad_fn=<SumBackward0>)\n",
      "102\n",
      "tensor(507.2157, grad_fn=<SumBackward0>)\n",
      "103\n",
      "tensor(481.0952, grad_fn=<SumBackward0>)\n",
      "104\n",
      "tensor(456.3939, grad_fn=<SumBackward0>)\n",
      "105\n",
      "tensor(433.0374, grad_fn=<SumBackward0>)\n",
      "106\n",
      "tensor(410.9440, grad_fn=<SumBackward0>)\n",
      "107\n",
      "tensor(390.0438, grad_fn=<SumBackward0>)\n",
      "108\n",
      "tensor(370.2591, grad_fn=<SumBackward0>)\n",
      "109\n",
      "tensor(351.5323, grad_fn=<SumBackward0>)\n",
      "110\n",
      "tensor(333.8088, grad_fn=<SumBackward0>)\n",
      "111\n",
      "tensor(317.0182, grad_fn=<SumBackward0>)\n",
      "112\n",
      "tensor(301.1266, grad_fn=<SumBackward0>)\n",
      "113\n",
      "tensor(286.0648, grad_fn=<SumBackward0>)\n",
      "114\n",
      "tensor(271.7943, grad_fn=<SumBackward0>)\n",
      "115\n",
      "tensor(258.2702, grad_fn=<SumBackward0>)\n",
      "116\n",
      "tensor(245.4513, grad_fn=<SumBackward0>)\n",
      "117\n",
      "tensor(233.3026, grad_fn=<SumBackward0>)\n",
      "118\n",
      "tensor(221.7841, grad_fn=<SumBackward0>)\n",
      "119\n",
      "tensor(210.8597, grad_fn=<SumBackward0>)\n",
      "120\n",
      "tensor(200.5023, grad_fn=<SumBackward0>)\n",
      "121\n",
      "tensor(190.6716, grad_fn=<SumBackward0>)\n",
      "122\n",
      "tensor(181.3481, grad_fn=<SumBackward0>)\n",
      "123\n",
      "tensor(172.5008, grad_fn=<SumBackward0>)\n",
      "124\n",
      "tensor(164.1017, grad_fn=<SumBackward0>)\n",
      "125\n",
      "tensor(156.1277, grad_fn=<SumBackward0>)\n",
      "126\n",
      "tensor(148.5587, grad_fn=<SumBackward0>)\n",
      "127\n",
      "tensor(141.3737, grad_fn=<SumBackward0>)\n",
      "128\n",
      "tensor(134.5488, grad_fn=<SumBackward0>)\n",
      "129\n",
      "tensor(128.0669, grad_fn=<SumBackward0>)\n",
      "130\n",
      "tensor(121.9123, grad_fn=<SumBackward0>)\n",
      "131\n",
      "tensor(116.0643, grad_fn=<SumBackward0>)\n",
      "132\n",
      "tensor(110.5085, grad_fn=<SumBackward0>)\n",
      "133\n",
      "tensor(105.2279, grad_fn=<SumBackward0>)\n",
      "134\n",
      "tensor(100.2086, grad_fn=<SumBackward0>)\n",
      "135\n",
      "tensor(95.4380, grad_fn=<SumBackward0>)\n",
      "136\n",
      "tensor(90.9030, grad_fn=<SumBackward0>)\n",
      "137\n",
      "tensor(86.5910, grad_fn=<SumBackward0>)\n",
      "138\n",
      "tensor(82.4904, grad_fn=<SumBackward0>)\n",
      "139\n",
      "tensor(78.5910, grad_fn=<SumBackward0>)\n",
      "140\n",
      "tensor(74.8813, grad_fn=<SumBackward0>)\n",
      "141\n",
      "tensor(71.3542, grad_fn=<SumBackward0>)\n",
      "142\n",
      "tensor(68.0003, grad_fn=<SumBackward0>)\n",
      "143\n",
      "tensor(64.8094, grad_fn=<SumBackward0>)\n",
      "144\n",
      "tensor(61.7716, grad_fn=<SumBackward0>)\n",
      "145\n",
      "tensor(58.8807, grad_fn=<SumBackward0>)\n",
      "146\n",
      "tensor(56.1285, grad_fn=<SumBackward0>)\n",
      "147\n",
      "tensor(53.5102, grad_fn=<SumBackward0>)\n",
      "148\n",
      "tensor(51.0177, grad_fn=<SumBackward0>)\n",
      "149\n",
      "tensor(48.6449, grad_fn=<SumBackward0>)\n",
      "150\n",
      "tensor(46.3860, grad_fn=<SumBackward0>)\n",
      "151\n",
      "tensor(44.2356, grad_fn=<SumBackward0>)\n",
      "152\n",
      "tensor(42.1875, grad_fn=<SumBackward0>)\n",
      "153\n",
      "tensor(40.2372, grad_fn=<SumBackward0>)\n",
      "154\n",
      "tensor(38.3804, grad_fn=<SumBackward0>)\n",
      "155\n",
      "tensor(36.6093, grad_fn=<SumBackward0>)\n",
      "156\n",
      "tensor(34.9244, grad_fn=<SumBackward0>)\n",
      "157\n",
      "tensor(33.3180, grad_fn=<SumBackward0>)\n",
      "158\n",
      "tensor(31.7882, grad_fn=<SumBackward0>)\n",
      "159\n",
      "tensor(30.3301, grad_fn=<SumBackward0>)\n",
      "160\n",
      "tensor(28.9413, grad_fn=<SumBackward0>)\n",
      "161\n",
      "tensor(27.6171, grad_fn=<SumBackward0>)\n",
      "162\n",
      "tensor(26.3553, grad_fn=<SumBackward0>)\n",
      "163\n",
      "tensor(25.1522, grad_fn=<SumBackward0>)\n",
      "164\n",
      "tensor(24.0068, grad_fn=<SumBackward0>)\n",
      "165\n",
      "tensor(22.9141, grad_fn=<SumBackward0>)\n",
      "166\n",
      "tensor(21.8723, grad_fn=<SumBackward0>)\n",
      "167\n",
      "tensor(20.8791, grad_fn=<SumBackward0>)\n",
      "168\n",
      "tensor(19.9319, grad_fn=<SumBackward0>)\n",
      "169\n",
      "tensor(19.0290, grad_fn=<SumBackward0>)\n",
      "170\n",
      "tensor(18.1677, grad_fn=<SumBackward0>)\n",
      "171\n",
      "tensor(17.3460, grad_fn=<SumBackward0>)\n",
      "172\n",
      "tensor(16.5626, grad_fn=<SumBackward0>)\n",
      "173\n",
      "tensor(15.8157, grad_fn=<SumBackward0>)\n",
      "174\n",
      "tensor(15.1035, grad_fn=<SumBackward0>)\n",
      "175\n",
      "tensor(14.4237, grad_fn=<SumBackward0>)\n",
      "176\n",
      "tensor(13.7752, grad_fn=<SumBackward0>)\n",
      "177\n",
      "tensor(13.1566, grad_fn=<SumBackward0>)\n",
      "178\n",
      "tensor(12.5656, grad_fn=<SumBackward0>)\n",
      "179\n",
      "tensor(12.0028, grad_fn=<SumBackward0>)\n",
      "180\n",
      "tensor(11.4654, grad_fn=<SumBackward0>)\n",
      "181\n",
      "tensor(10.9521, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n",
      "tensor(10.4629, grad_fn=<SumBackward0>)\n",
      "183\n",
      "tensor(9.9951, grad_fn=<SumBackward0>)\n",
      "184\n",
      "tensor(9.5495, grad_fn=<SumBackward0>)\n",
      "185\n",
      "tensor(9.1238, grad_fn=<SumBackward0>)\n",
      "186\n",
      "tensor(8.7173, grad_fn=<SumBackward0>)\n",
      "187\n",
      "tensor(8.3297, grad_fn=<SumBackward0>)\n",
      "188\n",
      "tensor(7.9595, grad_fn=<SumBackward0>)\n",
      "189\n",
      "tensor(7.6058, grad_fn=<SumBackward0>)\n",
      "190\n",
      "tensor(7.2682, grad_fn=<SumBackward0>)\n",
      "191\n",
      "tensor(6.9459, grad_fn=<SumBackward0>)\n",
      "192\n",
      "tensor(6.6384, grad_fn=<SumBackward0>)\n",
      "193\n",
      "tensor(6.3449, grad_fn=<SumBackward0>)\n",
      "194\n",
      "tensor(6.0640, grad_fn=<SumBackward0>)\n",
      "195\n",
      "tensor(5.7962, grad_fn=<SumBackward0>)\n",
      "196\n",
      "tensor(5.5402, grad_fn=<SumBackward0>)\n",
      "197\n",
      "tensor(5.2957, grad_fn=<SumBackward0>)\n",
      "198\n",
      "tensor(5.0623, grad_fn=<SumBackward0>)\n",
      "199\n",
      "tensor(4.8394, grad_fn=<SumBackward0>)\n",
      "200\n",
      "tensor(4.6261, grad_fn=<SumBackward0>)\n",
      "201\n",
      "tensor(4.4227, grad_fn=<SumBackward0>)\n",
      "202\n",
      "tensor(4.2281, grad_fn=<SumBackward0>)\n",
      "203\n",
      "tensor(4.0426, grad_fn=<SumBackward0>)\n",
      "204\n",
      "tensor(3.8654, grad_fn=<SumBackward0>)\n",
      "205\n",
      "tensor(3.6958, grad_fn=<SumBackward0>)\n",
      "206\n",
      "tensor(3.5338, grad_fn=<SumBackward0>)\n",
      "207\n",
      "tensor(3.3791, grad_fn=<SumBackward0>)\n",
      "208\n",
      "tensor(3.2312, grad_fn=<SumBackward0>)\n",
      "209\n",
      "tensor(3.0899, grad_fn=<SumBackward0>)\n",
      "210\n",
      "tensor(2.9550, grad_fn=<SumBackward0>)\n",
      "211\n",
      "tensor(2.8260, grad_fn=<SumBackward0>)\n",
      "212\n",
      "tensor(2.7026, grad_fn=<SumBackward0>)\n",
      "213\n",
      "tensor(2.5846, grad_fn=<SumBackward0>)\n",
      "214\n",
      "tensor(2.4720, grad_fn=<SumBackward0>)\n",
      "215\n",
      "tensor(2.3643, grad_fn=<SumBackward0>)\n",
      "216\n",
      "tensor(2.2615, grad_fn=<SumBackward0>)\n",
      "217\n",
      "tensor(2.1631, grad_fn=<SumBackward0>)\n",
      "218\n",
      "tensor(2.0691, grad_fn=<SumBackward0>)\n",
      "219\n",
      "tensor(1.9792, grad_fn=<SumBackward0>)\n",
      "220\n",
      "tensor(1.8934, grad_fn=<SumBackward0>)\n",
      "221\n",
      "tensor(1.8113, grad_fn=<SumBackward0>)\n",
      "222\n",
      "tensor(1.7326, grad_fn=<SumBackward0>)\n",
      "223\n",
      "tensor(1.6575, grad_fn=<SumBackward0>)\n",
      "224\n",
      "tensor(1.5857, grad_fn=<SumBackward0>)\n",
      "225\n",
      "tensor(1.5170, grad_fn=<SumBackward0>)\n",
      "226\n",
      "tensor(1.4515, grad_fn=<SumBackward0>)\n",
      "227\n",
      "tensor(1.3887, grad_fn=<SumBackward0>)\n",
      "228\n",
      "tensor(1.3287, grad_fn=<SumBackward0>)\n",
      "229\n",
      "tensor(1.2714, grad_fn=<SumBackward0>)\n",
      "230\n",
      "tensor(1.2165, grad_fn=<SumBackward0>)\n",
      "231\n",
      "tensor(1.1638, grad_fn=<SumBackward0>)\n",
      "232\n",
      "tensor(1.1138, grad_fn=<SumBackward0>)\n",
      "233\n",
      "tensor(1.0658, grad_fn=<SumBackward0>)\n",
      "234\n",
      "tensor(1.0199, grad_fn=<SumBackward0>)\n",
      "235\n",
      "tensor(0.9760, grad_fn=<SumBackward0>)\n",
      "236\n",
      "tensor(0.9340, grad_fn=<SumBackward0>)\n",
      "237\n",
      "tensor(0.8939, grad_fn=<SumBackward0>)\n",
      "238\n",
      "tensor(0.8555, grad_fn=<SumBackward0>)\n",
      "239\n",
      "tensor(0.8187, grad_fn=<SumBackward0>)\n",
      "240\n",
      "tensor(0.7836, grad_fn=<SumBackward0>)\n",
      "241\n",
      "tensor(0.7499, grad_fn=<SumBackward0>)\n",
      "242\n",
      "tensor(0.7177, grad_fn=<SumBackward0>)\n",
      "243\n",
      "tensor(0.6870, grad_fn=<SumBackward0>)\n",
      "244\n",
      "tensor(0.6576, grad_fn=<SumBackward0>)\n",
      "245\n",
      "tensor(0.6294, grad_fn=<SumBackward0>)\n",
      "246\n",
      "tensor(0.6024, grad_fn=<SumBackward0>)\n",
      "247\n",
      "tensor(0.5768, grad_fn=<SumBackward0>)\n",
      "248\n",
      "tensor(0.5521, grad_fn=<SumBackward0>)\n",
      "249\n",
      "tensor(0.5285, grad_fn=<SumBackward0>)\n",
      "250\n",
      "tensor(0.5059, grad_fn=<SumBackward0>)\n",
      "251\n",
      "tensor(0.4843, grad_fn=<SumBackward0>)\n",
      "252\n",
      "tensor(0.4636, grad_fn=<SumBackward0>)\n",
      "253\n",
      "tensor(0.4438, grad_fn=<SumBackward0>)\n",
      "254\n",
      "tensor(0.4249, grad_fn=<SumBackward0>)\n",
      "255\n",
      "tensor(0.4068, grad_fn=<SumBackward0>)\n",
      "256\n",
      "tensor(0.3895, grad_fn=<SumBackward0>)\n",
      "257\n",
      "tensor(0.3729, grad_fn=<SumBackward0>)\n",
      "258\n",
      "tensor(0.3570, grad_fn=<SumBackward0>)\n",
      "259\n",
      "tensor(0.3419, grad_fn=<SumBackward0>)\n",
      "260\n",
      "tensor(0.3273, grad_fn=<SumBackward0>)\n",
      "261\n",
      "tensor(0.3133, grad_fn=<SumBackward0>)\n",
      "262\n",
      "tensor(0.3001, grad_fn=<SumBackward0>)\n",
      "263\n",
      "tensor(0.2873, grad_fn=<SumBackward0>)\n",
      "264\n",
      "tensor(0.2751, grad_fn=<SumBackward0>)\n",
      "265\n",
      "tensor(0.2634, grad_fn=<SumBackward0>)\n",
      "266\n",
      "tensor(0.2523, grad_fn=<SumBackward0>)\n",
      "267\n",
      "tensor(0.2416, grad_fn=<SumBackward0>)\n",
      "268\n",
      "tensor(0.2313, grad_fn=<SumBackward0>)\n",
      "269\n",
      "tensor(0.2216, grad_fn=<SumBackward0>)\n",
      "270\n",
      "tensor(0.2122, grad_fn=<SumBackward0>)\n",
      "271\n",
      "tensor(0.2032, grad_fn=<SumBackward0>)\n",
      "272\n",
      "tensor(0.1946, grad_fn=<SumBackward0>)\n",
      "273\n",
      "tensor(0.1864, grad_fn=<SumBackward0>)\n",
      "274\n",
      "tensor(0.1785, grad_fn=<SumBackward0>)\n",
      "275\n",
      "tensor(0.1709, grad_fn=<SumBackward0>)\n",
      "276\n",
      "tensor(0.1637, grad_fn=<SumBackward0>)\n",
      "277\n",
      "tensor(0.1568, grad_fn=<SumBackward0>)\n",
      "278\n",
      "tensor(0.1502, grad_fn=<SumBackward0>)\n",
      "279\n",
      "tensor(0.1438, grad_fn=<SumBackward0>)\n",
      "280\n",
      "tensor(0.1377, grad_fn=<SumBackward0>)\n",
      "281\n",
      "tensor(0.1320, grad_fn=<SumBackward0>)\n",
      "282\n",
      "tensor(0.1264, grad_fn=<SumBackward0>)\n",
      "283\n",
      "tensor(0.1211, grad_fn=<SumBackward0>)\n",
      "284\n",
      "tensor(0.1160, grad_fn=<SumBackward0>)\n",
      "285\n",
      "tensor(0.1111, grad_fn=<SumBackward0>)\n",
      "286\n",
      "tensor(0.1065, grad_fn=<SumBackward0>)\n",
      "287\n",
      "tensor(0.1020, grad_fn=<SumBackward0>)\n",
      "288\n",
      "tensor(0.0977, grad_fn=<SumBackward0>)\n",
      "289\n",
      "tensor(0.0936, grad_fn=<SumBackward0>)\n",
      "290\n",
      "tensor(0.0897, grad_fn=<SumBackward0>)\n",
      "291\n",
      "tensor(0.0859, grad_fn=<SumBackward0>)\n",
      "292\n",
      "tensor(0.0823, grad_fn=<SumBackward0>)\n",
      "293\n",
      "tensor(0.0788, grad_fn=<SumBackward0>)\n",
      "294\n",
      "tensor(0.0755, grad_fn=<SumBackward0>)\n",
      "295\n",
      "tensor(0.0724, grad_fn=<SumBackward0>)\n",
      "296\n",
      "tensor(0.0693, grad_fn=<SumBackward0>)\n",
      "297\n",
      "tensor(0.0664, grad_fn=<SumBackward0>)\n",
      "298\n",
      "tensor(0.0637, grad_fn=<SumBackward0>)\n",
      "299\n",
      "tensor(0.0610, grad_fn=<SumBackward0>)\n",
      "300\n",
      "tensor(0.0585, grad_fn=<SumBackward0>)\n",
      "301\n",
      "tensor(0.0560, grad_fn=<SumBackward0>)\n",
      "302\n",
      "tensor(0.0537, grad_fn=<SumBackward0>)\n",
      "303\n",
      "tensor(0.0514, grad_fn=<SumBackward0>)\n",
      "304\n",
      "tensor(0.0493, grad_fn=<SumBackward0>)\n",
      "305\n",
      "tensor(0.0472, grad_fn=<SumBackward0>)\n",
      "306\n",
      "tensor(0.0452, grad_fn=<SumBackward0>)\n",
      "307\n",
      "tensor(0.0434, grad_fn=<SumBackward0>)\n",
      "308\n",
      "tensor(0.0416, grad_fn=<SumBackward0>)\n",
      "309\n",
      "tensor(0.0398, grad_fn=<SumBackward0>)\n",
      "310\n",
      "tensor(0.0382, grad_fn=<SumBackward0>)\n",
      "311\n",
      "tensor(0.0366, grad_fn=<SumBackward0>)\n",
      "312\n",
      "tensor(0.0351, grad_fn=<SumBackward0>)\n",
      "313\n",
      "tensor(0.0336, grad_fn=<SumBackward0>)\n",
      "314\n",
      "tensor(0.0322, grad_fn=<SumBackward0>)\n",
      "315\n",
      "tensor(0.0309, grad_fn=<SumBackward0>)\n",
      "316\n",
      "tensor(0.0296, grad_fn=<SumBackward0>)\n",
      "317\n",
      "tensor(0.0284, grad_fn=<SumBackward0>)\n",
      "318\n",
      "tensor(0.0272, grad_fn=<SumBackward0>)\n",
      "319\n",
      "tensor(0.0261, grad_fn=<SumBackward0>)\n",
      "320\n",
      "tensor(0.0250, grad_fn=<SumBackward0>)\n",
      "321\n",
      "tensor(0.0240, grad_fn=<SumBackward0>)\n",
      "322\n",
      "tensor(0.0230, grad_fn=<SumBackward0>)\n",
      "323\n",
      "tensor(0.0220, grad_fn=<SumBackward0>)\n",
      "324\n",
      "tensor(0.0211, grad_fn=<SumBackward0>)\n",
      "325\n",
      "tensor(0.0203, grad_fn=<SumBackward0>)\n",
      "326\n",
      "tensor(0.0194, grad_fn=<SumBackward0>)\n",
      "327\n",
      "tensor(0.0186, grad_fn=<SumBackward0>)\n",
      "328\n",
      "tensor(0.0179, grad_fn=<SumBackward0>)\n",
      "329\n",
      "tensor(0.0172, grad_fn=<SumBackward0>)\n",
      "330\n",
      "tensor(0.0165, grad_fn=<SumBackward0>)\n",
      "331\n",
      "tensor(0.0158, grad_fn=<SumBackward0>)\n",
      "332\n",
      "tensor(0.0151, grad_fn=<SumBackward0>)\n",
      "333\n",
      "tensor(0.0145, grad_fn=<SumBackward0>)\n",
      "334\n",
      "tensor(0.0139, grad_fn=<SumBackward0>)\n",
      "335\n",
      "tensor(0.0134, grad_fn=<SumBackward0>)\n",
      "336\n",
      "tensor(0.0128, grad_fn=<SumBackward0>)\n",
      "337\n",
      "tensor(0.0123, grad_fn=<SumBackward0>)\n",
      "338\n",
      "tensor(0.0118, grad_fn=<SumBackward0>)\n",
      "339\n",
      "tensor(0.0113, grad_fn=<SumBackward0>)\n",
      "340\n",
      "tensor(0.0109, grad_fn=<SumBackward0>)\n",
      "341\n",
      "tensor(0.0104, grad_fn=<SumBackward0>)\n",
      "342\n",
      "tensor(0.0100, grad_fn=<SumBackward0>)\n",
      "343\n",
      "tensor(0.0096, grad_fn=<SumBackward0>)\n",
      "344\n",
      "tensor(0.0092, grad_fn=<SumBackward0>)\n",
      "345\n",
      "tensor(0.0089, grad_fn=<SumBackward0>)\n",
      "346\n",
      "tensor(0.0085, grad_fn=<SumBackward0>)\n",
      "347\n",
      "tensor(0.0082, grad_fn=<SumBackward0>)\n",
      "348\n",
      "tensor(0.0079, grad_fn=<SumBackward0>)\n",
      "349\n",
      "tensor(0.0075, grad_fn=<SumBackward0>)\n",
      "350\n",
      "tensor(0.0073, grad_fn=<SumBackward0>)\n",
      "351\n",
      "tensor(0.0070, grad_fn=<SumBackward0>)\n",
      "352\n",
      "tensor(0.0067, grad_fn=<SumBackward0>)\n",
      "353\n",
      "tensor(0.0064, grad_fn=<SumBackward0>)\n",
      "354\n",
      "tensor(0.0062, grad_fn=<SumBackward0>)\n",
      "355\n",
      "tensor(0.0059, grad_fn=<SumBackward0>)\n",
      "356\n",
      "tensor(0.0057, grad_fn=<SumBackward0>)\n",
      "357\n",
      "tensor(0.0055, grad_fn=<SumBackward0>)\n",
      "358\n",
      "tensor(0.0053, grad_fn=<SumBackward0>)\n",
      "359\n",
      "tensor(0.0051, grad_fn=<SumBackward0>)\n",
      "360\n",
      "tensor(0.0049, grad_fn=<SumBackward0>)\n",
      "361\n",
      "tensor(0.0047, grad_fn=<SumBackward0>)\n",
      "362\n",
      "tensor(0.0045, grad_fn=<SumBackward0>)\n",
      "363\n",
      "tensor(0.0043, grad_fn=<SumBackward0>)\n",
      "364\n",
      "tensor(0.0042, grad_fn=<SumBackward0>)\n",
      "365\n",
      "tensor(0.0040, grad_fn=<SumBackward0>)\n",
      "366\n",
      "tensor(0.0039, grad_fn=<SumBackward0>)\n",
      "367\n",
      "tensor(0.0037, grad_fn=<SumBackward0>)\n",
      "368\n",
      "tensor(0.0036, grad_fn=<SumBackward0>)\n",
      "369\n",
      "tensor(0.0034, grad_fn=<SumBackward0>)\n",
      "370\n",
      "tensor(0.0033, grad_fn=<SumBackward0>)\n",
      "371\n",
      "tensor(0.0032, grad_fn=<SumBackward0>)\n",
      "372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0031, grad_fn=<SumBackward0>)\n",
      "373\n",
      "tensor(0.0030, grad_fn=<SumBackward0>)\n",
      "374\n",
      "tensor(0.0029, grad_fn=<SumBackward0>)\n",
      "375\n",
      "tensor(0.0028, grad_fn=<SumBackward0>)\n",
      "376\n",
      "tensor(0.0027, grad_fn=<SumBackward0>)\n",
      "377\n",
      "tensor(0.0026, grad_fn=<SumBackward0>)\n",
      "378\n",
      "tensor(0.0025, grad_fn=<SumBackward0>)\n",
      "379\n",
      "tensor(0.0024, grad_fn=<SumBackward0>)\n",
      "380\n",
      "tensor(0.0023, grad_fn=<SumBackward0>)\n",
      "381\n",
      "tensor(0.0022, grad_fn=<SumBackward0>)\n",
      "382\n",
      "tensor(0.0021, grad_fn=<SumBackward0>)\n",
      "383\n",
      "tensor(0.0021, grad_fn=<SumBackward0>)\n",
      "384\n",
      "tensor(0.0020, grad_fn=<SumBackward0>)\n",
      "385\n",
      "tensor(0.0019, grad_fn=<SumBackward0>)\n",
      "386\n",
      "tensor(0.0019, grad_fn=<SumBackward0>)\n",
      "387\n",
      "tensor(0.0018, grad_fn=<SumBackward0>)\n",
      "388\n",
      "tensor(0.0017, grad_fn=<SumBackward0>)\n",
      "389\n",
      "tensor(0.0017, grad_fn=<SumBackward0>)\n",
      "390\n",
      "tensor(0.0016, grad_fn=<SumBackward0>)\n",
      "391\n",
      "tensor(0.0016, grad_fn=<SumBackward0>)\n",
      "392\n",
      "tensor(0.0015, grad_fn=<SumBackward0>)\n",
      "393\n",
      "tensor(0.0015, grad_fn=<SumBackward0>)\n",
      "394\n",
      "tensor(0.0014, grad_fn=<SumBackward0>)\n",
      "395\n",
      "tensor(0.0014, grad_fn=<SumBackward0>)\n",
      "396\n",
      "tensor(0.0013, grad_fn=<SumBackward0>)\n",
      "397\n",
      "tensor(0.0013, grad_fn=<SumBackward0>)\n",
      "398\n",
      "tensor(0.0012, grad_fn=<SumBackward0>)\n",
      "399\n",
      "tensor(0.0012, grad_fn=<SumBackward0>)\n",
      "400\n",
      "tensor(0.0012, grad_fn=<SumBackward0>)\n",
      "401\n",
      "tensor(0.0011, grad_fn=<SumBackward0>)\n",
      "402\n",
      "tensor(0.0011, grad_fn=<SumBackward0>)\n",
      "403\n",
      "tensor(0.0011, grad_fn=<SumBackward0>)\n",
      "404\n",
      "tensor(0.0010, grad_fn=<SumBackward0>)\n",
      "405\n",
      "tensor(0.0010, grad_fn=<SumBackward0>)\n",
      "406\n",
      "tensor(0.0010, grad_fn=<SumBackward0>)\n",
      "407\n",
      "tensor(0.0009, grad_fn=<SumBackward0>)\n",
      "408\n",
      "tensor(0.0009, grad_fn=<SumBackward0>)\n",
      "409\n",
      "tensor(0.0009, grad_fn=<SumBackward0>)\n",
      "410\n",
      "tensor(0.0009, grad_fn=<SumBackward0>)\n",
      "411\n",
      "tensor(0.0008, grad_fn=<SumBackward0>)\n",
      "412\n",
      "tensor(0.0008, grad_fn=<SumBackward0>)\n",
      "413\n",
      "tensor(0.0008, grad_fn=<SumBackward0>)\n",
      "414\n",
      "tensor(0.0008, grad_fn=<SumBackward0>)\n",
      "415\n",
      "tensor(0.0007, grad_fn=<SumBackward0>)\n",
      "416\n",
      "tensor(0.0007, grad_fn=<SumBackward0>)\n",
      "417\n",
      "tensor(0.0007, grad_fn=<SumBackward0>)\n",
      "418\n",
      "tensor(0.0007, grad_fn=<SumBackward0>)\n",
      "419\n",
      "tensor(0.0007, grad_fn=<SumBackward0>)\n",
      "420\n",
      "tensor(0.0006, grad_fn=<SumBackward0>)\n",
      "421\n",
      "tensor(0.0006, grad_fn=<SumBackward0>)\n",
      "422\n",
      "tensor(0.0006, grad_fn=<SumBackward0>)\n",
      "423\n",
      "tensor(0.0006, grad_fn=<SumBackward0>)\n",
      "424\n",
      "tensor(0.0006, grad_fn=<SumBackward0>)\n",
      "425\n",
      "tensor(0.0006, grad_fn=<SumBackward0>)\n",
      "426\n",
      "tensor(0.0005, grad_fn=<SumBackward0>)\n",
      "427\n",
      "tensor(0.0005, grad_fn=<SumBackward0>)\n",
      "428\n",
      "tensor(0.0005, grad_fn=<SumBackward0>)\n",
      "429\n",
      "tensor(0.0005, grad_fn=<SumBackward0>)\n",
      "430\n",
      "tensor(0.0005, grad_fn=<SumBackward0>)\n",
      "431\n",
      "tensor(0.0005, grad_fn=<SumBackward0>)\n",
      "432\n",
      "tensor(0.0005, grad_fn=<SumBackward0>)\n",
      "433\n",
      "tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "434\n",
      "tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "435\n",
      "tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "436\n",
      "tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "437\n",
      "tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "438\n",
      "tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "439\n",
      "tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "440\n",
      "tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "441\n",
      "tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "442\n",
      "tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "443\n",
      "tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "444\n",
      "tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "445\n",
      "tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "446\n",
      "tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "447\n",
      "tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "448\n",
      "tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "449\n",
      "tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "450\n",
      "tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "451\n",
      "tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "452\n",
      "tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "453\n",
      "tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "454\n",
      "tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "455\n",
      "tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "456\n",
      "tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "457\n",
      "tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "458\n",
      "tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "459\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "460\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "461\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "462\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "463\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "464\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "465\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "466\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "467\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "468\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "469\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "470\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "471\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "472\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "473\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "474\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "475\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "476\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "477\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "478\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "479\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "480\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "481\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "482\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "483\n",
      "tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "484\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "485\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "486\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "487\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "488\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "489\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "490\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "491\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "492\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "493\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "494\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "495\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "496\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "497\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "498\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "499\n",
      "tensor(0.0001, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor #GPU에서 실행하려면 이 주석을 제거하세요\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원\n",
    "# H는 은닉 계층의 차원이며, D_out은 출력 차원\n",
    "\n",
    "N,D_in,H,D_out = 64,1000,100,10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor 를 생성하고, Variable로 감쌉니다.\n",
    "# requires_grade = False로 설정하여 역전파중에 이 Variable들에 대한 변화도를 계산할 필요가 없음을 나타냅니다.\n",
    "x = Variable(torch.randn(N,D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N,D_out).type(dtype), requires_grad=False)\n",
    "\n",
    "# 가중치를 저장하기 위해 무작위 값을 갖는 Tensor를 생성하고, Variable로 감쌉니다.\n",
    "# requires_grad = True 로 설정하여 역전파 중에 이 Variable들에 대한 변화도를 계산할 필요가 있음을 나타냅니다.\n",
    "\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype),requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype),requires_grad=True)\n",
    "\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500) :\n",
    "    # 순전파 단계 : Variable 연산을 사용하여 y 값을 예측합니다. 이는 Tensor를 사용한\n",
    "    # 순전파 단계와 완전히 동일하지만, 역전파 단계를 별도로 구현하지 않기 위함입니다.\n",
    "    # 값들 (Intermediate Value)에 대한 참조(Reference) 를 갖고 있을 필요가 없습니다.\n",
    "    \n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # Variable 연산을 사용하여 손실을 계한하고 출력합니다.\n",
    "    # Loss 는 (1,) 모양을 갖는 Variable이며, Loss.data는 (1,) 모양의 Tensor입니다.\n",
    "    # Loss.data[0]는 손실(Loss)의 스칼라 값입니다.\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t)\n",
    "    print(loss)\n",
    "    \n",
    "    # autograde를 사용하여 역전파 단계를 계산합니다. 이는 requires_grad = True를\n",
    "    # 갖는 모든 Variable에 대한 손실의 변화도를 계산합니다. 이후 w1.grad와 w2.grad는 \n",
    "    # w1과 w2 각각에 대한 손실의 변화도를 갖는 Variable이 됩니다.\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # 경사하강법(Gradient Descent) 을 사용하여 가중치를 갱신합니다. \n",
    "    # w1.data 와 w2.data는 Tensor이며, w1.grad와 w2.grad는 Variable이고, \n",
    "    # w1.grad.data와 w2.grad.data 또한 Tensor입니다.\n",
    "    \n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "    \n",
    "    # 가중치 갱신 후에는 수동으로 변화도를 0으로 만들어줍니다.\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch : 새 autograd 함수 정의하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, autograd의 기본(primitive)연산자는 실제로 Tensor를 조작하는 2개의 함수입니다. foward함수는 입력 Tensor로부터 출력 Tensor를 계산합니다. backward함수는 출력 Tensor의 변화도를 받고 입력 Tensor의 변화도를 계산합니다.  \n",
    "\n",
    "Pytorch에서 torch.autograd.Function의 서브클래스를 정의하고 foward와 backward함수를 구현함으로써 쉽게 사용자 정의 autograd연산자를 정의할 수 있습니다. 그 후 , 인스턴스(instance)를 생성하고 함수처럼 호출하여 입력 데이터를 포함하는 Variable을 전달하는 식으로 새로운 autograd 연산자를 쉽게 사용할 수 있습니다. \n",
    "\n",
    "이 예제에서는 ReLU 비선형성(nonlinearity)을 수행하기 위한 사용자 정의 autograd함수를 정희하고, 2-계층 신경망에 이를 적용해 보도록 하겠습니다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(30190504.)\n",
      "1 tensor(28334216.)\n",
      "2 tensor(31392220.)\n",
      "3 tensor(34249056.)\n",
      "4 tensor(31809564.)\n",
      "5 tensor(23565708.)\n",
      "6 tensor(13724072.)\n",
      "7 tensor(6880701.5000)\n",
      "8 tensor(3389391.7500)\n",
      "9 tensor(1859037.3750)\n",
      "10 tensor(1191202.8750)\n",
      "11 tensor(870814.2500)\n",
      "12 tensor(691008.2500)\n",
      "13 tensor(572969.5000)\n",
      "14 tensor(486089.9688)\n",
      "15 tensor(417671.5312)\n",
      "16 tensor(361755.7500)\n",
      "17 tensor(315197.1875)\n",
      "18 tensor(275898.7188)\n",
      "19 tensor(242445.4375)\n",
      "20 tensor(213795.4375)\n",
      "21 tensor(189165.8438)\n",
      "22 tensor(167884.6406)\n",
      "23 tensor(149383.0938)\n",
      "24 tensor(133247.9219)\n",
      "25 tensor(119120.0938)\n",
      "26 tensor(106722.6875)\n",
      "27 tensor(95808.1172)\n",
      "28 tensor(86166.4844)\n",
      "29 tensor(77637.6250)\n",
      "30 tensor(70074.2500)\n",
      "31 tensor(63343.5391)\n",
      "32 tensor(57348.2969)\n",
      "33 tensor(51996.8789)\n",
      "34 tensor(47208.3516)\n",
      "35 tensor(42917.2188)\n",
      "36 tensor(39065.9961)\n",
      "37 tensor(35603.1328)\n",
      "38 tensor(32485.8711)\n",
      "39 tensor(29676.1406)\n",
      "40 tensor(27138.0273)\n",
      "41 tensor(24847.4961)\n",
      "42 tensor(22772.9082)\n",
      "43 tensor(20892.0801)\n",
      "44 tensor(19185.5371)\n",
      "45 tensor(17637.0820)\n",
      "46 tensor(16227.4434)\n",
      "47 tensor(14943.6963)\n",
      "48 tensor(13773.2910)\n",
      "49 tensor(12705.0068)\n",
      "50 tensor(11728.0830)\n",
      "51 tensor(10834.2090)\n",
      "52 tensor(10016.1982)\n",
      "53 tensor(9265.9180)\n",
      "54 tensor(8578.1162)\n",
      "55 tensor(7946.5195)\n",
      "56 tensor(7366.0693)\n",
      "57 tensor(6832.5459)\n",
      "58 tensor(6341.7168)\n",
      "59 tensor(5889.4697)\n",
      "60 tensor(5472.9473)\n",
      "61 tensor(5088.4595)\n",
      "62 tensor(4733.8486)\n",
      "63 tensor(4406.3242)\n",
      "64 tensor(4103.8789)\n",
      "65 tensor(3824.2192)\n",
      "66 tensor(3565.3105)\n",
      "67 tensor(3325.6758)\n",
      "68 tensor(3103.6704)\n",
      "69 tensor(2897.8811)\n",
      "70 tensor(2707.1309)\n",
      "71 tensor(2530.1282)\n",
      "72 tensor(2365.7363)\n",
      "73 tensor(2213.0491)\n",
      "74 tensor(2071.0996)\n",
      "75 tensor(1939.1035)\n",
      "76 tensor(1816.2971)\n",
      "77 tensor(1701.9773)\n",
      "78 tensor(1595.4875)\n",
      "79 tensor(1496.2313)\n",
      "80 tensor(1403.7274)\n",
      "81 tensor(1317.4523)\n",
      "82 tensor(1236.9850)\n",
      "83 tensor(1161.8586)\n",
      "84 tensor(1091.6678)\n",
      "85 tensor(1026.0735)\n",
      "86 tensor(964.7942)\n",
      "87 tensor(907.4646)\n",
      "88 tensor(853.8268)\n",
      "89 tensor(803.6932)\n",
      "90 tensor(756.7332)\n",
      "91 tensor(712.7732)\n",
      "92 tensor(671.5591)\n",
      "93 tensor(632.9231)\n",
      "94 tensor(596.7013)\n",
      "95 tensor(562.6985)\n",
      "96 tensor(530.7814)\n",
      "97 tensor(500.8322)\n",
      "98 tensor(472.6933)\n",
      "99 tensor(446.2489)\n",
      "100 tensor(421.4127)\n",
      "101 tensor(398.0536)\n",
      "102 tensor(376.0749)\n",
      "103 tensor(355.4071)\n",
      "104 tensor(335.9504)\n",
      "105 tensor(317.6295)\n",
      "106 tensor(300.3938)\n",
      "107 tensor(284.1488)\n",
      "108 tensor(268.8373)\n",
      "109 tensor(254.4202)\n",
      "110 tensor(240.8194)\n",
      "111 tensor(227.9904)\n",
      "112 tensor(215.8983)\n",
      "113 tensor(204.4816)\n",
      "114 tensor(193.7161)\n",
      "115 tensor(183.5787)\n",
      "116 tensor(174.0027)\n",
      "117 tensor(164.9587)\n",
      "118 tensor(156.4145)\n",
      "119 tensor(148.3426)\n",
      "120 tensor(140.7117)\n",
      "121 tensor(133.4934)\n",
      "122 tensor(126.6699)\n",
      "123 tensor(120.2129)\n",
      "124 tensor(114.1010)\n",
      "125 tensor(108.3239)\n",
      "126 tensor(102.8498)\n",
      "127 tensor(97.6685)\n",
      "128 tensor(92.7652)\n",
      "129 tensor(88.1178)\n",
      "130 tensor(83.7179)\n",
      "131 tensor(79.5458)\n",
      "132 tensor(75.5949)\n",
      "133 tensor(71.8474)\n",
      "134 tensor(68.2940)\n",
      "135 tensor(64.9263)\n",
      "136 tensor(61.7333)\n",
      "137 tensor(58.7049)\n",
      "138 tensor(55.8307)\n",
      "139 tensor(53.1053)\n",
      "140 tensor(50.5174)\n",
      "141 tensor(48.0621)\n",
      "142 tensor(45.7314)\n",
      "143 tensor(43.5186)\n",
      "144 tensor(41.4165)\n",
      "145 tensor(39.4210)\n",
      "146 tensor(37.5261)\n",
      "147 tensor(35.7255)\n",
      "148 tensor(34.0141)\n",
      "149 tensor(32.3883)\n",
      "150 tensor(30.8432)\n",
      "151 tensor(29.3753)\n",
      "152 tensor(27.9797)\n",
      "153 tensor(26.6526)\n",
      "154 tensor(25.3913)\n",
      "155 tensor(24.1915)\n",
      "156 tensor(23.0506)\n",
      "157 tensor(21.9654)\n",
      "158 tensor(20.9331)\n",
      "159 tensor(19.9505)\n",
      "160 tensor(19.0164)\n",
      "161 tensor(18.1272)\n",
      "162 tensor(17.2808)\n",
      "163 tensor(16.4758)\n",
      "164 tensor(15.7095)\n",
      "165 tensor(14.9790)\n",
      "166 tensor(14.2842)\n",
      "167 tensor(13.6230)\n",
      "168 tensor(12.9928)\n",
      "169 tensor(12.3922)\n",
      "170 tensor(11.8209)\n",
      "171 tensor(11.2768)\n",
      "172 tensor(10.7588)\n",
      "173 tensor(10.2648)\n",
      "174 tensor(9.7940)\n",
      "175 tensor(9.3457)\n",
      "176 tensor(8.9184)\n",
      "177 tensor(8.5105)\n",
      "178 tensor(8.1224)\n",
      "179 tensor(7.7522)\n",
      "180 tensor(7.3996)\n",
      "181 tensor(7.0637)\n",
      "182 tensor(6.7428)\n",
      "183 tensor(6.4376)\n",
      "184 tensor(6.1462)\n",
      "185 tensor(5.8680)\n",
      "186 tensor(5.6028)\n",
      "187 tensor(5.3503)\n",
      "188 tensor(5.1094)\n",
      "189 tensor(4.8795)\n",
      "190 tensor(4.6598)\n",
      "191 tensor(4.4510)\n",
      "192 tensor(4.2514)\n",
      "193 tensor(4.0608)\n",
      "194 tensor(3.8791)\n",
      "195 tensor(3.7056)\n",
      "196 tensor(3.5402)\n",
      "197 tensor(3.3824)\n",
      "198 tensor(3.2316)\n",
      "199 tensor(3.0876)\n",
      "200 tensor(2.9504)\n",
      "201 tensor(2.8192)\n",
      "202 tensor(2.6943)\n",
      "203 tensor(2.5749)\n",
      "204 tensor(2.4608)\n",
      "205 tensor(2.3519)\n",
      "206 tensor(2.2481)\n",
      "207 tensor(2.1487)\n",
      "208 tensor(2.0539)\n",
      "209 tensor(1.9633)\n",
      "210 tensor(1.8769)\n",
      "211 tensor(1.7944)\n",
      "212 tensor(1.7156)\n",
      "213 tensor(1.6402)\n",
      "214 tensor(1.5681)\n",
      "215 tensor(1.4995)\n",
      "216 tensor(1.4338)\n",
      "217 tensor(1.3710)\n",
      "218 tensor(1.3108)\n",
      "219 tensor(1.2538)\n",
      "220 tensor(1.1991)\n",
      "221 tensor(1.1467)\n",
      "222 tensor(1.0968)\n",
      "223 tensor(1.0490)\n",
      "224 tensor(1.0033)\n",
      "225 tensor(0.9597)\n",
      "226 tensor(0.9180)\n",
      "227 tensor(0.8782)\n",
      "228 tensor(0.8401)\n",
      "229 tensor(0.8036)\n",
      "230 tensor(0.7689)\n",
      "231 tensor(0.7355)\n",
      "232 tensor(0.7037)\n",
      "233 tensor(0.6733)\n",
      "234 tensor(0.6442)\n",
      "235 tensor(0.6165)\n",
      "236 tensor(0.5899)\n",
      "237 tensor(0.5644)\n",
      "238 tensor(0.5401)\n",
      "239 tensor(0.5168)\n",
      "240 tensor(0.4946)\n",
      "241 tensor(0.4734)\n",
      "242 tensor(0.4530)\n",
      "243 tensor(0.4335)\n",
      "244 tensor(0.4150)\n",
      "245 tensor(0.3972)\n",
      "246 tensor(0.3802)\n",
      "247 tensor(0.3639)\n",
      "248 tensor(0.3483)\n",
      "249 tensor(0.3333)\n",
      "250 tensor(0.3191)\n",
      "251 tensor(0.3055)\n",
      "252 tensor(0.2924)\n",
      "253 tensor(0.2799)\n",
      "254 tensor(0.2680)\n",
      "255 tensor(0.2565)\n",
      "256 tensor(0.2456)\n",
      "257 tensor(0.2352)\n",
      "258 tensor(0.2252)\n",
      "259 tensor(0.2156)\n",
      "260 tensor(0.2065)\n",
      "261 tensor(0.1977)\n",
      "262 tensor(0.1893)\n",
      "263 tensor(0.1813)\n",
      "264 tensor(0.1736)\n",
      "265 tensor(0.1662)\n",
      "266 tensor(0.1592)\n",
      "267 tensor(0.1524)\n",
      "268 tensor(0.1460)\n",
      "269 tensor(0.1398)\n",
      "270 tensor(0.1339)\n",
      "271 tensor(0.1282)\n",
      "272 tensor(0.1228)\n",
      "273 tensor(0.1176)\n",
      "274 tensor(0.1127)\n",
      "275 tensor(0.1079)\n",
      "276 tensor(0.1034)\n",
      "277 tensor(0.0990)\n",
      "278 tensor(0.0948)\n",
      "279 tensor(0.0909)\n",
      "280 tensor(0.0871)\n",
      "281 tensor(0.0834)\n",
      "282 tensor(0.0799)\n",
      "283 tensor(0.0765)\n",
      "284 tensor(0.0733)\n",
      "285 tensor(0.0702)\n",
      "286 tensor(0.0673)\n",
      "287 tensor(0.0645)\n",
      "288 tensor(0.0618)\n",
      "289 tensor(0.0592)\n",
      "290 tensor(0.0567)\n",
      "291 tensor(0.0543)\n",
      "292 tensor(0.0521)\n",
      "293 tensor(0.0499)\n",
      "294 tensor(0.0478)\n",
      "295 tensor(0.0458)\n",
      "296 tensor(0.0439)\n",
      "297 tensor(0.0421)\n",
      "298 tensor(0.0403)\n",
      "299 tensor(0.0387)\n",
      "300 tensor(0.0371)\n",
      "301 tensor(0.0355)\n",
      "302 tensor(0.0341)\n",
      "303 tensor(0.0326)\n",
      "304 tensor(0.0313)\n",
      "305 tensor(0.0300)\n",
      "306 tensor(0.0287)\n",
      "307 tensor(0.0276)\n",
      "308 tensor(0.0264)\n",
      "309 tensor(0.0253)\n",
      "310 tensor(0.0243)\n",
      "311 tensor(0.0233)\n",
      "312 tensor(0.0223)\n",
      "313 tensor(0.0214)\n",
      "314 tensor(0.0205)\n",
      "315 tensor(0.0197)\n",
      "316 tensor(0.0189)\n",
      "317 tensor(0.0181)\n",
      "318 tensor(0.0174)\n",
      "319 tensor(0.0167)\n",
      "320 tensor(0.0160)\n",
      "321 tensor(0.0153)\n",
      "322 tensor(0.0147)\n",
      "323 tensor(0.0141)\n",
      "324 tensor(0.0135)\n",
      "325 tensor(0.0130)\n",
      "326 tensor(0.0124)\n",
      "327 tensor(0.0119)\n",
      "328 tensor(0.0115)\n",
      "329 tensor(0.0110)\n",
      "330 tensor(0.0106)\n",
      "331 tensor(0.0101)\n",
      "332 tensor(0.0097)\n",
      "333 tensor(0.0093)\n",
      "334 tensor(0.0090)\n",
      "335 tensor(0.0086)\n",
      "336 tensor(0.0083)\n",
      "337 tensor(0.0079)\n",
      "338 tensor(0.0076)\n",
      "339 tensor(0.0073)\n",
      "340 tensor(0.0070)\n",
      "341 tensor(0.0068)\n",
      "342 tensor(0.0065)\n",
      "343 tensor(0.0062)\n",
      "344 tensor(0.0060)\n",
      "345 tensor(0.0058)\n",
      "346 tensor(0.0055)\n",
      "347 tensor(0.0053)\n",
      "348 tensor(0.0051)\n",
      "349 tensor(0.0049)\n",
      "350 tensor(0.0047)\n",
      "351 tensor(0.0046)\n",
      "352 tensor(0.0044)\n",
      "353 tensor(0.0042)\n",
      "354 tensor(0.0041)\n",
      "355 tensor(0.0039)\n",
      "356 tensor(0.0038)\n",
      "357 tensor(0.0036)\n",
      "358 tensor(0.0035)\n",
      "359 tensor(0.0034)\n",
      "360 tensor(0.0032)\n",
      "361 tensor(0.0031)\n",
      "362 tensor(0.0030)\n",
      "363 tensor(0.0029)\n",
      "364 tensor(0.0028)\n",
      "365 tensor(0.0027)\n",
      "366 tensor(0.0026)\n",
      "367 tensor(0.0025)\n",
      "368 tensor(0.0024)\n",
      "369 tensor(0.0023)\n",
      "370 tensor(0.0022)\n",
      "371 tensor(0.0022)\n",
      "372 tensor(0.0021)\n",
      "373 tensor(0.0020)\n",
      "374 tensor(0.0019)\n",
      "375 tensor(0.0019)\n",
      "376 tensor(0.0018)\n",
      "377 tensor(0.0017)\n",
      "378 tensor(0.0017)\n",
      "379 tensor(0.0016)\n",
      "380 tensor(0.0016)\n",
      "381 tensor(0.0015)\n",
      "382 tensor(0.0015)\n",
      "383 tensor(0.0014)\n",
      "384 tensor(0.0014)\n",
      "385 tensor(0.0013)\n",
      "386 tensor(0.0013)\n",
      "387 tensor(0.0013)\n",
      "388 tensor(0.0012)\n",
      "389 tensor(0.0012)\n",
      "390 tensor(0.0011)\n",
      "391 tensor(0.0011)\n",
      "392 tensor(0.0011)\n",
      "393 tensor(0.0010)\n",
      "394 tensor(0.0010)\n",
      "395 tensor(0.0010)\n",
      "396 tensor(0.0009)\n",
      "397 tensor(0.0009)\n",
      "398 tensor(0.0009)\n",
      "399 tensor(0.0009)\n",
      "400 tensor(0.0008)\n",
      "401 tensor(0.0008)\n",
      "402 tensor(0.0008)\n",
      "403 tensor(0.0008)\n",
      "404 tensor(0.0007)\n",
      "405 tensor(0.0007)\n",
      "406 tensor(0.0007)\n",
      "407 tensor(0.0007)\n",
      "408 tensor(0.0007)\n",
      "409 tensor(0.0006)\n",
      "410 tensor(0.0006)\n",
      "411 tensor(0.0006)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412 tensor(0.0006)\n",
      "413 tensor(0.0006)\n",
      "414 tensor(0.0006)\n",
      "415 tensor(0.0006)\n",
      "416 tensor(0.0005)\n",
      "417 tensor(0.0005)\n",
      "418 tensor(0.0005)\n",
      "419 tensor(0.0005)\n",
      "420 tensor(0.0005)\n",
      "421 tensor(0.0005)\n",
      "422 tensor(0.0005)\n",
      "423 tensor(0.0004)\n",
      "424 tensor(0.0004)\n",
      "425 tensor(0.0004)\n",
      "426 tensor(0.0004)\n",
      "427 tensor(0.0004)\n",
      "428 tensor(0.0004)\n",
      "429 tensor(0.0004)\n",
      "430 tensor(0.0004)\n",
      "431 tensor(0.0004)\n",
      "432 tensor(0.0004)\n",
      "433 tensor(0.0003)\n",
      "434 tensor(0.0003)\n",
      "435 tensor(0.0003)\n",
      "436 tensor(0.0003)\n",
      "437 tensor(0.0003)\n",
      "438 tensor(0.0003)\n",
      "439 tensor(0.0003)\n",
      "440 tensor(0.0003)\n",
      "441 tensor(0.0003)\n",
      "442 tensor(0.0003)\n",
      "443 tensor(0.0003)\n",
      "444 tensor(0.0003)\n",
      "445 tensor(0.0003)\n",
      "446 tensor(0.0003)\n",
      "447 tensor(0.0003)\n",
      "448 tensor(0.0003)\n",
      "449 tensor(0.0002)\n",
      "450 tensor(0.0002)\n",
      "451 tensor(0.0002)\n",
      "452 tensor(0.0002)\n",
      "453 tensor(0.0002)\n",
      "454 tensor(0.0002)\n",
      "455 tensor(0.0002)\n",
      "456 tensor(0.0002)\n",
      "457 tensor(0.0002)\n",
      "458 tensor(0.0002)\n",
      "459 tensor(0.0002)\n",
      "460 tensor(0.0002)\n",
      "461 tensor(0.0002)\n",
      "462 tensor(0.0002)\n",
      "463 tensor(0.0002)\n",
      "464 tensor(0.0002)\n",
      "465 tensor(0.0002)\n",
      "466 tensor(0.0002)\n",
      "467 tensor(0.0002)\n",
      "468 tensor(0.0002)\n",
      "469 tensor(0.0002)\n",
      "470 tensor(0.0002)\n",
      "471 tensor(0.0002)\n",
      "472 tensor(0.0002)\n",
      "473 tensor(0.0002)\n",
      "474 tensor(0.0002)\n",
      "475 tensor(0.0001)\n",
      "476 tensor(0.0001)\n",
      "477 tensor(0.0001)\n",
      "478 tensor(0.0001)\n",
      "479 tensor(0.0001)\n",
      "480 tensor(0.0001)\n",
      "481 tensor(0.0001)\n",
      "482 tensor(0.0001)\n",
      "483 tensor(0.0001)\n",
      "484 tensor(0.0001)\n",
      "485 tensor(0.0001)\n",
      "486 tensor(0.0001)\n",
      "487 tensor(0.0001)\n",
      "488 tensor(0.0001)\n",
      "489 tensor(0.0001)\n",
      "490 tensor(0.0001)\n",
      "491 tensor(0.0001)\n",
      "492 tensor(0.0001)\n",
      "493 tensor(0.0001)\n",
      "494 tensor(0.0001)\n",
      "495 tensor(0.0001)\n",
      "496 tensor(0.0001)\n",
      "497 tensor(0.0001)\n",
      "498 tensor(0.0001)\n",
      "499 tensor(0.0001)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class MyReLU(torch.autograd.Function) :\n",
    "    \"\"\"\n",
    "    torch.autograd.Function을 상속받아 사용자 정의 autograd 함수를 구현하고,\n",
    "    Tensor 연산을 하는 순전파와 역전파 단계를 구현해보자!!\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input) :\n",
    "        \"\"\"\n",
    "        순전파 단계에서는 입력을 갖는 Tensor를 받아 출력을 갖는 Tensor를 반환합니다.\n",
    "        ctx는 역전파 연산을 위한 정보를 저장하기 위해 사용하는 Context Object입니다.\n",
    "        ctx.save_for_backward method를 사용하여 역전파 단계에서 사용할!!어떠한 \n",
    "        객체 (object) 도 저장 (cache)해 둘 수 있습니다.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output) :\n",
    "        \"\"\"\n",
    "        역전파 단계에서는 출력에 대한 손실의 변화도를 갖는 Tensor를 받고, 입력에 대한 손실의 변화도를 계산합니다.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "    \n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor # GPU에서 실행하려면 이 주석을 제거하세요.\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉 계층의 차원이며, D_out은 출력 차원입니다:\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성하고, Variable로 감쌉니다.\n",
    "\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "\n",
    "# 가중치를 저장하기 위해 무작위 값을 갖는 Tensor를 생성하고, Variable로\n",
    "# 감쌉니다.\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    \"\"\"\n",
    "    여기 집중 위와 달라짐\n",
    "    \"\"\"\n",
    "    # 사용자 정의 함수를 적용하기 위해 Function.apply method를 사용합니다.\n",
    "    # 이를 'relu'라고 이름(alias) 붙였습니다.\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # 순전파 단계: Variable 연산을 사용하여 y 값을 예측합니다;\n",
    "    # 사용자 정의 autograd 연산을 사용하여 ReLU를 계산합니다.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # 손실(loss)을 계산하고 출력합니다.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.data)\n",
    "\n",
    "    # autograde를 사용하여 역전파 단계를 계산합니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사하강법(Gradient Descent)을 사용하여 가중치를 갱신합니다.\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "\n",
    "    # 가중치 갱신 후에는 수동으로 변화도를 0으로 만듭니다.\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow : 정적 그래프 (Static Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch autograd는 Tensorflow와 많이 닮아 보입니다. 두 프레임워크 모두 연산 그래프를 정의하며, 자동 미분을 사용하여 변화도를 계산합니다. 두 프레임워크의 가장 큰 차이점은 Tensorflow의 연산 그래프는 Tensorflow 연산의 그래프느 ㄴ정적이며, Pytorch는 동적그래프를 사용합니다.  \n",
    "\n",
    "Tensorflow에서는 연산 그래프를 한 번 정의한 후 동일한 그래프를 계속해서 실행하며 가능한 다른 입력 데이터를 전달합니다. Pytorch에서는 각각의 순전파 단게에서 새로운 연산 그래프를 정의합니다.  \n",
    "\n",
    "정적 그래프는 먼저(Up-front) 그래프를 최적화 할 수 있기 때문에 좋습니다. 예를 들어 프레임워크가 효율을 위해 일부 그래프 연산을 합치거나, 여러 GPU나 시스템에 그래프를 배포하는 전략을 제시할 수 있습니다. 만약 동일한 그래프를 계속 재사용하면, 같은 그래프가 반복되면서 비싼 최적화 비용을 잠재적으로 상환할 수 있습니다.  \n",
    "\n",
    "정적 그래프와 동적 그래프는 제어 흐름(Control flow)측면에서도 다릅니다. 어떤 모델에서 각 데이터 지점마다 다른 연산 연산을 수행하고 싶을 수 있습니다. 예를 들어 순환 신경망에서 각각의 데이터 지점마다 서로 다른 횟수만큼 펼칠수 있습니다. 이러한 펼침은 반복문 Loop로 구현이 가능합니다. 정적 그래프에서 반복문은 그래프의 일부가 돼야 합니다. 이러한 이유는 Tensorflow는 그래프 내에 반복문을 포함하기 위해 tf.scan과 같은 연산자를 제공합니다. 동적 그래프에서는 이러한 상황이 더 단순해 집니다.  각 예제에 대한 그래프를 즉석에서 작성하기 때문에, 동적 그래프에서는 이러한 상황이 더 단순합니다. 각 예제에 대한 그래프를 즉성(on the fly)에서 작성하기 떄문에 일반적인 명령혁(imperative)제어 흐름을 사용하여 각각의 입력에 따라 다른 계산을 수행할 수 있습니다.  \n",
    "위 Pytorch autograd 예제들과는 다르게 Tensorflow를 사용하여 2계층 신경망을 구성하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1028 00:25:31.558885 13376 deprecation.py:323] From C:\\Users\\ashgh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37365444.0\n",
      "39971130.0\n",
      "51104520.0\n",
      "66525110.0\n",
      "125327210.0\n",
      "447432600.0\n",
      "1542089600.0\n",
      "6407015000.0\n",
      "5389994000.0\n",
      "1375581500000.0\n",
      "1.02887934e+18\n",
      "1.1878478e+36\n",
      "inf\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 먼저 연산 그래프를 구성하겠습니다:\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉 계층의 차원이며, D_out은 출력 차원입니다:\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 정답(target) 데이터를 위한 Placeholder를 생성합니다; 이는 우리가 그래프를\n",
    "# 실행할 때 실제 데이터로 채워질 것입니다.\n",
    "x = tf.placeholder(tf.float32, shape=(None, D_in))\n",
    "y = tf.placeholder(tf.float32, shape=(None, D_out))\n",
    "\n",
    "# 가중치를 저장하기 위한 Variable을 생성하고 무작위 데이터로 초기화합니다.\n",
    "# Tensorflow의 Variable은 그래프가 실행되는 동안 그 값이 유지됩니다.\n",
    "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D_out)))\n",
    "\n",
    "# 순전파 단계: Tensorflow의 Tensor 연산을 사용하여 y 값을 예측합니다.\n",
    "# 이 코드가 어떠한 수치 연산을 실제로 수행하지는 않는다는 것을 유의하세요;\n",
    "# 이 단계에서는 나중에 실행할 연산 그래프를 구성하기만 합니다.\n",
    "h = tf.matmul(x, w1)\n",
    "h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, w2)\n",
    "\n",
    "# Tensorflow의 Tensor 연산을 사용하여 손실(loss)을 계산합니다.\n",
    "loss = tf.reduce_sum((y - y_pred) ** 2.0)\n",
    "\n",
    "# 손실에 따른 w1, w2의 변화도(Gradient)를 계산합니다.\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "\n",
    "# 경사하강법(Gradient Descent)을 사용하여 가중치를 갱신합니다. 실제로 가중치를\n",
    "# 갱신하기 위해서는 그래프가 실행될 때 new_w1과 new_w2 계산(evaluate)해야 합니다.\n",
    "# Tensorflow에서 가중치의 값을 갱신하는 작업은 연산 그래프의 일부임을 유의하십시오;\n",
    "# PyTorch에서는 이 작업이 연산 그래프의 밖에서 일어납니다.\n",
    "learning_rate = 1e-6\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "\n",
    "# 지금까지 우리는 연산 그래프를 구성하였으므로, 실제로 그래프를 실행하기 위해 이제\n",
    "# Tensorflow 세션(Session)에 들어가보겠습니다.\n",
    "with tf.Session() as sess:\n",
    "    # 그래프를 한 번 실행하여 Variable w1과 w2를 초기화합니다.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # 입력 데이터 x와 정답 데이터 y를 저장하기 위한 NumPy 배열을 생성합니다.\n",
    "    x_value = np.random.randn(N, D_in)\n",
    "    y_value = np.random.randn(N, D_out)\n",
    "    for _ in range(500):\n",
    "        # 그래프를 여러 번 실행합니다. 매번 그래프가 실행할 때마다 feed_dict\n",
    "        # 인자(argument)로 명시하여 x_value를 x에, y_value를 y에 할당(bind)하고자\n",
    "        # 합니다. 또한, 그래프를 실행할 때마다 손실과 new_w1, new_w2 값을\n",
    "        # 계산하려고 합니다; 이러한 Tensor들의 값은 NumPy 배열로 반환됩니다.\n",
    "        loss_value, _, _ = sess.run([loss, new_w1, new_w2],\n",
    "                                    feed_dict={x: x_value, y: y_value})\n",
    "        print(loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn 모듈\n",
    "\n",
    "### Pytorch : nn\n",
    "\n",
    "연산 그래프와 autograd는 복잡한 연산자를 정의하고 도함수 (derivative)를 자동으로 계산하는데 매우 강력한 패러다임(paradigm)입니다. 하지만 규모가 큰 신경망에서는 autograd 그 자체만으로는 너무 낮은 수준일 수 있습니다. \n",
    "\n",
    "신경망을 구성할 때 종종 연산을 여러 계층 으로 배열하는 것으로 생각하게 되는데, 이중 일부는 학습도중 최적화가ㅏ 될 학습 가능한 매개변수를 갖고 있습니다.   \n",
    "\n",
    "Tensorflow 에서 Keras, TensorFlow-Slim,나 TFLearn 같은 패키지는 원초적인 연산 그래프보다 더 높은 수준의 추상화를 제공하여 신경망을 구축하는데 있어 유용합니다.  \n",
    "\n",
    "Pytorch에선 nn패키지가 동일한 기능을 제공합니다. nn패키지는 대략 신경망 계층들과 동일한 모듈의 집합을 정의합니다.  모듈은 입력 Variable을 받고 출력 Variable을 계산하는 한편, 학습 가능한 매개변수를 포함하는 Variable과 같은 내부 상태(internal state)를 갖습니다. 또한 nn패키지는 신경망을 학습시킬 때 주로 사용하는 유용한 손실 함수들도 정의합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(680.2180)\n",
      "1 tensor(631.7173)\n",
      "2 tensor(589.7276)\n",
      "3 tensor(553.4382)\n",
      "4 tensor(520.8665)\n",
      "5 tensor(491.5690)\n",
      "6 tensor(464.9676)\n",
      "7 tensor(440.5335)\n",
      "8 tensor(417.9570)\n",
      "9 tensor(397.0676)\n",
      "10 tensor(377.4464)\n",
      "11 tensor(358.9196)\n",
      "12 tensor(341.3716)\n",
      "13 tensor(324.7317)\n",
      "14 tensor(308.8842)\n",
      "15 tensor(293.6805)\n",
      "16 tensor(279.1624)\n",
      "17 tensor(265.3292)\n",
      "18 tensor(252.1141)\n",
      "19 tensor(239.4323)\n",
      "20 tensor(227.2652)\n",
      "21 tensor(215.5932)\n",
      "22 tensor(204.4180)\n",
      "23 tensor(193.7264)\n",
      "24 tensor(183.4973)\n",
      "25 tensor(173.7191)\n",
      "26 tensor(164.4105)\n",
      "27 tensor(155.5480)\n",
      "28 tensor(147.1022)\n",
      "29 tensor(139.0709)\n",
      "30 tensor(131.4287)\n",
      "31 tensor(124.1679)\n",
      "32 tensor(117.2948)\n",
      "33 tensor(110.7769)\n",
      "34 tensor(104.6021)\n",
      "35 tensor(98.7648)\n",
      "36 tensor(93.2394)\n",
      "37 tensor(88.0140)\n",
      "38 tensor(83.0841)\n",
      "39 tensor(78.4275)\n",
      "40 tensor(74.0373)\n",
      "41 tensor(69.8993)\n",
      "42 tensor(65.9902)\n",
      "43 tensor(62.3054)\n",
      "44 tensor(58.8296)\n",
      "45 tensor(55.5383)\n",
      "46 tensor(52.4422)\n",
      "47 tensor(49.5305)\n",
      "48 tensor(46.7888)\n",
      "49 tensor(44.2091)\n",
      "50 tensor(41.7795)\n",
      "51 tensor(39.4842)\n",
      "52 tensor(37.3264)\n",
      "53 tensor(35.2959)\n",
      "54 tensor(33.3808)\n",
      "55 tensor(31.5759)\n",
      "56 tensor(29.8775)\n",
      "57 tensor(28.2761)\n",
      "58 tensor(26.7671)\n",
      "59 tensor(25.3468)\n",
      "60 tensor(24.0054)\n",
      "61 tensor(22.7405)\n",
      "62 tensor(21.5448)\n",
      "63 tensor(20.4165)\n",
      "64 tensor(19.3533)\n",
      "65 tensor(18.3490)\n",
      "66 tensor(17.4020)\n",
      "67 tensor(16.5073)\n",
      "68 tensor(15.6630)\n",
      "69 tensor(14.8635)\n",
      "70 tensor(14.1087)\n",
      "71 tensor(13.3966)\n",
      "72 tensor(12.7234)\n",
      "73 tensor(12.0865)\n",
      "74 tensor(11.4861)\n",
      "75 tensor(10.9198)\n",
      "76 tensor(10.3836)\n",
      "77 tensor(9.8761)\n",
      "78 tensor(9.3957)\n",
      "79 tensor(8.9374)\n",
      "80 tensor(8.5033)\n",
      "81 tensor(8.0918)\n",
      "82 tensor(7.7012)\n",
      "83 tensor(7.3314)\n",
      "84 tensor(6.9807)\n",
      "85 tensor(6.6482)\n",
      "86 tensor(6.3325)\n",
      "87 tensor(6.0331)\n",
      "88 tensor(5.7488)\n",
      "89 tensor(5.4788)\n",
      "90 tensor(5.2233)\n",
      "91 tensor(4.9812)\n",
      "92 tensor(4.7514)\n",
      "93 tensor(4.5334)\n",
      "94 tensor(4.3261)\n",
      "95 tensor(4.1289)\n",
      "96 tensor(3.9415)\n",
      "97 tensor(3.7633)\n",
      "98 tensor(3.5944)\n",
      "99 tensor(3.4334)\n",
      "100 tensor(3.2804)\n",
      "101 tensor(3.1352)\n",
      "102 tensor(2.9971)\n",
      "103 tensor(2.8656)\n",
      "104 tensor(2.7401)\n",
      "105 tensor(2.6208)\n",
      "106 tensor(2.5074)\n",
      "107 tensor(2.3994)\n",
      "108 tensor(2.2965)\n",
      "109 tensor(2.1985)\n",
      "110 tensor(2.1051)\n",
      "111 tensor(2.0158)\n",
      "112 tensor(1.9307)\n",
      "113 tensor(1.8495)\n",
      "114 tensor(1.7722)\n",
      "115 tensor(1.6984)\n",
      "116 tensor(1.6278)\n",
      "117 tensor(1.5604)\n",
      "118 tensor(1.4960)\n",
      "119 tensor(1.4345)\n",
      "120 tensor(1.3758)\n",
      "121 tensor(1.3196)\n",
      "122 tensor(1.2660)\n",
      "123 tensor(1.2147)\n",
      "124 tensor(1.1657)\n",
      "125 tensor(1.1189)\n",
      "126 tensor(1.0741)\n",
      "127 tensor(1.0313)\n",
      "128 tensor(0.9904)\n",
      "129 tensor(0.9512)\n",
      "130 tensor(0.9136)\n",
      "131 tensor(0.8777)\n",
      "132 tensor(0.8433)\n",
      "133 tensor(0.8103)\n",
      "134 tensor(0.7788)\n",
      "135 tensor(0.7485)\n",
      "136 tensor(0.7196)\n",
      "137 tensor(0.6918)\n",
      "138 tensor(0.6652)\n",
      "139 tensor(0.6397)\n",
      "140 tensor(0.6152)\n",
      "141 tensor(0.5918)\n",
      "142 tensor(0.5693)\n",
      "143 tensor(0.5478)\n",
      "144 tensor(0.5271)\n",
      "145 tensor(0.5073)\n",
      "146 tensor(0.4883)\n",
      "147 tensor(0.4700)\n",
      "148 tensor(0.4525)\n",
      "149 tensor(0.4357)\n",
      "150 tensor(0.4195)\n",
      "151 tensor(0.4040)\n",
      "152 tensor(0.3891)\n",
      "153 tensor(0.3748)\n",
      "154 tensor(0.3610)\n",
      "155 tensor(0.3478)\n",
      "156 tensor(0.3351)\n",
      "157 tensor(0.3229)\n",
      "158 tensor(0.3112)\n",
      "159 tensor(0.2999)\n",
      "160 tensor(0.2891)\n",
      "161 tensor(0.2786)\n",
      "162 tensor(0.2686)\n",
      "163 tensor(0.2590)\n",
      "164 tensor(0.2497)\n",
      "165 tensor(0.2408)\n",
      "166 tensor(0.2322)\n",
      "167 tensor(0.2239)\n",
      "168 tensor(0.2160)\n",
      "169 tensor(0.2083)\n",
      "170 tensor(0.2010)\n",
      "171 tensor(0.1939)\n",
      "172 tensor(0.1870)\n",
      "173 tensor(0.1805)\n",
      "174 tensor(0.1741)\n",
      "175 tensor(0.1680)\n",
      "176 tensor(0.1622)\n",
      "177 tensor(0.1565)\n",
      "178 tensor(0.1511)\n",
      "179 tensor(0.1458)\n",
      "180 tensor(0.1408)\n",
      "181 tensor(0.1359)\n",
      "182 tensor(0.1313)\n",
      "183 tensor(0.1268)\n",
      "184 tensor(0.1224)\n",
      "185 tensor(0.1182)\n",
      "186 tensor(0.1142)\n",
      "187 tensor(0.1103)\n",
      "188 tensor(0.1066)\n",
      "189 tensor(0.1030)\n",
      "190 tensor(0.0995)\n",
      "191 tensor(0.0961)\n",
      "192 tensor(0.0929)\n",
      "193 tensor(0.0897)\n",
      "194 tensor(0.0867)\n",
      "195 tensor(0.0838)\n",
      "196 tensor(0.0810)\n",
      "197 tensor(0.0783)\n",
      "198 tensor(0.0757)\n",
      "199 tensor(0.0732)\n",
      "200 tensor(0.0707)\n",
      "201 tensor(0.0684)\n",
      "202 tensor(0.0661)\n",
      "203 tensor(0.0639)\n",
      "204 tensor(0.0618)\n",
      "205 tensor(0.0598)\n",
      "206 tensor(0.0578)\n",
      "207 tensor(0.0559)\n",
      "208 tensor(0.0541)\n",
      "209 tensor(0.0523)\n",
      "210 tensor(0.0506)\n",
      "211 tensor(0.0490)\n",
      "212 tensor(0.0474)\n",
      "213 tensor(0.0458)\n",
      "214 tensor(0.0443)\n",
      "215 tensor(0.0429)\n",
      "216 tensor(0.0415)\n",
      "217 tensor(0.0402)\n",
      "218 tensor(0.0389)\n",
      "219 tensor(0.0376)\n",
      "220 tensor(0.0364)\n",
      "221 tensor(0.0352)\n",
      "222 tensor(0.0341)\n",
      "223 tensor(0.0330)\n",
      "224 tensor(0.0320)\n",
      "225 tensor(0.0309)\n",
      "226 tensor(0.0300)\n",
      "227 tensor(0.0290)\n",
      "228 tensor(0.0281)\n",
      "229 tensor(0.0272)\n",
      "230 tensor(0.0263)\n",
      "231 tensor(0.0255)\n",
      "232 tensor(0.0247)\n",
      "233 tensor(0.0239)\n",
      "234 tensor(0.0231)\n",
      "235 tensor(0.0224)\n",
      "236 tensor(0.0217)\n",
      "237 tensor(0.0210)\n",
      "238 tensor(0.0204)\n",
      "239 tensor(0.0197)\n",
      "240 tensor(0.0191)\n",
      "241 tensor(0.0185)\n",
      "242 tensor(0.0179)\n",
      "243 tensor(0.0174)\n",
      "244 tensor(0.0168)\n",
      "245 tensor(0.0163)\n",
      "246 tensor(0.0158)\n",
      "247 tensor(0.0153)\n",
      "248 tensor(0.0148)\n",
      "249 tensor(0.0144)\n",
      "250 tensor(0.0139)\n",
      "251 tensor(0.0135)\n",
      "252 tensor(0.0131)\n",
      "253 tensor(0.0127)\n",
      "254 tensor(0.0123)\n",
      "255 tensor(0.0119)\n",
      "256 tensor(0.0115)\n",
      "257 tensor(0.0112)\n",
      "258 tensor(0.0108)\n",
      "259 tensor(0.0105)\n",
      "260 tensor(0.0102)\n",
      "261 tensor(0.0099)\n",
      "262 tensor(0.0096)\n",
      "263 tensor(0.0093)\n",
      "264 tensor(0.0090)\n",
      "265 tensor(0.0087)\n",
      "266 tensor(0.0085)\n",
      "267 tensor(0.0082)\n",
      "268 tensor(0.0080)\n",
      "269 tensor(0.0077)\n",
      "270 tensor(0.0075)\n",
      "271 tensor(0.0072)\n",
      "272 tensor(0.0070)\n",
      "273 tensor(0.0068)\n",
      "274 tensor(0.0066)\n",
      "275 tensor(0.0064)\n",
      "276 tensor(0.0062)\n",
      "277 tensor(0.0060)\n",
      "278 tensor(0.0058)\n",
      "279 tensor(0.0057)\n",
      "280 tensor(0.0055)\n",
      "281 tensor(0.0053)\n",
      "282 tensor(0.0052)\n",
      "283 tensor(0.0050)\n",
      "284 tensor(0.0049)\n",
      "285 tensor(0.0047)\n",
      "286 tensor(0.0046)\n",
      "287 tensor(0.0044)\n",
      "288 tensor(0.0043)\n",
      "289 tensor(0.0042)\n",
      "290 tensor(0.0041)\n",
      "291 tensor(0.0039)\n",
      "292 tensor(0.0038)\n",
      "293 tensor(0.0037)\n",
      "294 tensor(0.0036)\n",
      "295 tensor(0.0035)\n",
      "296 tensor(0.0034)\n",
      "297 tensor(0.0033)\n",
      "298 tensor(0.0032)\n",
      "299 tensor(0.0031)\n",
      "300 tensor(0.0030)\n",
      "301 tensor(0.0029)\n",
      "302 tensor(0.0028)\n",
      "303 tensor(0.0027)\n",
      "304 tensor(0.0027)\n",
      "305 tensor(0.0026)\n",
      "306 tensor(0.0025)\n",
      "307 tensor(0.0024)\n",
      "308 tensor(0.0024)\n",
      "309 tensor(0.0023)\n",
      "310 tensor(0.0022)\n",
      "311 tensor(0.0022)\n",
      "312 tensor(0.0021)\n",
      "313 tensor(0.0020)\n",
      "314 tensor(0.0020)\n",
      "315 tensor(0.0019)\n",
      "316 tensor(0.0019)\n",
      "317 tensor(0.0018)\n",
      "318 tensor(0.0018)\n",
      "319 tensor(0.0017)\n",
      "320 tensor(0.0017)\n",
      "321 tensor(0.0016)\n",
      "322 tensor(0.0016)\n",
      "323 tensor(0.0015)\n",
      "324 tensor(0.0015)\n",
      "325 tensor(0.0014)\n",
      "326 tensor(0.0014)\n",
      "327 tensor(0.0013)\n",
      "328 tensor(0.0013)\n",
      "329 tensor(0.0013)\n",
      "330 tensor(0.0012)\n",
      "331 tensor(0.0012)\n",
      "332 tensor(0.0012)\n",
      "333 tensor(0.0011)\n",
      "334 tensor(0.0011)\n",
      "335 tensor(0.0011)\n",
      "336 tensor(0.0010)\n",
      "337 tensor(0.0010)\n",
      "338 tensor(0.0010)\n",
      "339 tensor(0.0009)\n",
      "340 tensor(0.0009)\n",
      "341 tensor(0.0009)\n",
      "342 tensor(0.0009)\n",
      "343 tensor(0.0008)\n",
      "344 tensor(0.0008)\n",
      "345 tensor(0.0008)\n",
      "346 tensor(0.0008)\n",
      "347 tensor(0.0007)\n",
      "348 tensor(0.0007)\n",
      "349 tensor(0.0007)\n",
      "350 tensor(0.0007)\n",
      "351 tensor(0.0007)\n",
      "352 tensor(0.0006)\n",
      "353 tensor(0.0006)\n",
      "354 tensor(0.0006)\n",
      "355 tensor(0.0006)\n",
      "356 tensor(0.0006)\n",
      "357 tensor(0.0006)\n",
      "358 tensor(0.0005)\n",
      "359 tensor(0.0005)\n",
      "360 tensor(0.0005)\n",
      "361 tensor(0.0005)\n",
      "362 tensor(0.0005)\n",
      "363 tensor(0.0005)\n",
      "364 tensor(0.0005)\n",
      "365 tensor(0.0004)\n",
      "366 tensor(0.0004)\n",
      "367 tensor(0.0004)\n",
      "368 tensor(0.0004)\n",
      "369 tensor(0.0004)\n",
      "370 tensor(0.0004)\n",
      "371 tensor(0.0004)\n",
      "372 tensor(0.0004)\n",
      "373 tensor(0.0004)\n",
      "374 tensor(0.0003)\n",
      "375 tensor(0.0003)\n",
      "376 tensor(0.0003)\n",
      "377 tensor(0.0003)\n",
      "378 tensor(0.0003)\n",
      "379 tensor(0.0003)\n",
      "380 tensor(0.0003)\n",
      "381 tensor(0.0003)\n",
      "382 tensor(0.0003)\n",
      "383 tensor(0.0003)\n",
      "384 tensor(0.0003)\n",
      "385 tensor(0.0003)\n",
      "386 tensor(0.0002)\n",
      "387 tensor(0.0002)\n",
      "388 tensor(0.0002)\n",
      "389 tensor(0.0002)\n",
      "390 tensor(0.0002)\n",
      "391 tensor(0.0002)\n",
      "392 tensor(0.0002)\n",
      "393 tensor(0.0002)\n",
      "394 tensor(0.0002)\n",
      "395 tensor(0.0002)\n",
      "396 tensor(0.0002)\n",
      "397 tensor(0.0002)\n",
      "398 tensor(0.0002)\n",
      "399 tensor(0.0002)\n",
      "400 tensor(0.0002)\n",
      "401 tensor(0.0002)\n",
      "402 tensor(0.0002)\n",
      "403 tensor(0.0002)\n",
      "404 tensor(0.0001)\n",
      "405 tensor(0.0001)\n",
      "406 tensor(0.0001)\n",
      "407 tensor(0.0001)\n",
      "408 tensor(0.0001)\n",
      "409 tensor(0.0001)\n",
      "410 tensor(0.0001)\n",
      "411 tensor(0.0001)\n",
      "412 tensor(0.0001)\n",
      "413 tensor(0.0001)\n",
      "414 tensor(0.0001)\n",
      "415 tensor(0.0001)\n",
      "416 tensor(0.0001)\n",
      "417 tensor(0.0001)\n",
      "418 tensor(9.9302e-05)\n",
      "419 tensor(9.6556e-05)\n",
      "420 tensor(9.3882e-05)\n",
      "421 tensor(9.1289e-05)\n",
      "422 tensor(8.8763e-05)\n",
      "423 tensor(8.6313e-05)\n",
      "424 tensor(8.3932e-05)\n",
      "425 tensor(8.1615e-05)\n",
      "426 tensor(7.9359e-05)\n",
      "427 tensor(7.7176e-05)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428 tensor(7.5046e-05)\n",
      "429 tensor(7.2979e-05)\n",
      "430 tensor(7.0962e-05)\n",
      "431 tensor(6.9011e-05)\n",
      "432 tensor(6.7108e-05)\n",
      "433 tensor(6.5261e-05)\n",
      "434 tensor(6.3465e-05)\n",
      "435 tensor(6.1719e-05)\n",
      "436 tensor(6.0020e-05)\n",
      "437 tensor(5.8369e-05)\n",
      "438 tensor(5.6762e-05)\n",
      "439 tensor(5.5202e-05)\n",
      "440 tensor(5.3686e-05)\n",
      "441 tensor(5.2210e-05)\n",
      "442 tensor(5.0776e-05)\n",
      "443 tensor(4.9382e-05)\n",
      "444 tensor(4.8026e-05)\n",
      "445 tensor(4.6708e-05)\n",
      "446 tensor(4.5426e-05)\n",
      "447 tensor(4.4179e-05)\n",
      "448 tensor(4.2968e-05)\n",
      "449 tensor(4.1791e-05)\n",
      "450 tensor(4.0645e-05)\n",
      "451 tensor(3.9532e-05)\n",
      "452 tensor(3.8445e-05)\n",
      "453 tensor(3.7392e-05)\n",
      "454 tensor(3.6369e-05)\n",
      "455 tensor(3.5375e-05)\n",
      "456 tensor(3.4408e-05)\n",
      "457 tensor(3.3464e-05)\n",
      "458 tensor(3.2549e-05)\n",
      "459 tensor(3.1660e-05)\n",
      "460 tensor(3.0793e-05)\n",
      "461 tensor(2.9953e-05)\n",
      "462 tensor(2.9133e-05)\n",
      "463 tensor(2.8338e-05)\n",
      "464 tensor(2.7564e-05)\n",
      "465 tensor(2.6813e-05)\n",
      "466 tensor(2.6082e-05)\n",
      "467 tensor(2.5370e-05)\n",
      "468 tensor(2.4677e-05)\n",
      "469 tensor(2.4003e-05)\n",
      "470 tensor(2.3348e-05)\n",
      "471 tensor(2.2712e-05)\n",
      "472 tensor(2.2093e-05)\n",
      "473 tensor(2.1492e-05)\n",
      "474 tensor(2.0905e-05)\n",
      "475 tensor(2.0335e-05)\n",
      "476 tensor(1.9781e-05)\n",
      "477 tensor(1.9242e-05)\n",
      "478 tensor(1.8720e-05)\n",
      "479 tensor(1.8210e-05)\n",
      "480 tensor(1.7714e-05)\n",
      "481 tensor(1.7232e-05)\n",
      "482 tensor(1.6762e-05)\n",
      "483 tensor(1.6307e-05)\n",
      "484 tensor(1.5864e-05)\n",
      "485 tensor(1.5434e-05)\n",
      "486 tensor(1.5013e-05)\n",
      "487 tensor(1.4606e-05)\n",
      "488 tensor(1.4208e-05)\n",
      "489 tensor(1.3821e-05)\n",
      "490 tensor(1.3446e-05)\n",
      "491 tensor(1.3081e-05)\n",
      "492 tensor(1.2727e-05)\n",
      "493 tensor(1.2382e-05)\n",
      "494 tensor(1.2047e-05)\n",
      "495 tensor(1.1719e-05)\n",
      "496 tensor(1.1401e-05)\n",
      "497 tensor(1.1091e-05)\n",
      "498 tensor(1.0791e-05)\n",
      "499 tensor(1.0499e-05)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성하고 Variable로 감쌉니다.\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out),requires_grad=False)\n",
    "\n",
    "# nn 패키지를 사용하여 모델을 순차적인 계층(sequence of Layers)로 정의합니다.\n",
    "# nn.sequential 은 다른 모듈들을 포함하는 모듈로, 그 모듈들을 순차적으로 적용하여 출력을 생성합니다. \n",
    "# 각각의 선형(Linear) 모듈은 선형 함수를 사용하여 입력으로 부터 출력을 계산하고, 가중치와 편향을 저장하기 위한 내부적인 Variable을 갖습니다.\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# 또한, nn 패키지에는 널리 사용하는 손실 함수들에 대한 정의도 포함하고 있습니다.\n",
    "# 여기에서는 평균 제곱 오차(MSE; Mean Squard Error)를 손실 함수로 사용하겠습니다.\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "learning_rate = 1e-4 \n",
    "for t in range(500) :\n",
    "    # 순전파 단계 : 모델에 x를 전달하여 예쌍하는 y값을 계산,\n",
    "    # 모듈 객체는 __call__연산자를 덮어써서(Override) 함수처럼 호출할 수 있게 합니다.\n",
    "    # 그렇게 함으로써 입력 데이터의 Variable을 모듈에 전달하고 출력 데이터의 Variable을 생성합니다.\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    \n",
    "    # 손실을 계산하고 출력합니다. 예측한 y값과 정답 y를 갖는 Variable들을 전달하고,\n",
    "    # 손실 함수는 손실(loss)를 갖는Variable을 반환합니다.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.data)\n",
    "    \n",
    "    # 역전파 단계를 실행하기 전에 변화도를 0으로 만듭니다.\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # 역전파 단계 : 모델의 학습 가능한 모든 매개변수에 대해서 손실의 변화도를 계산합니다\n",
    "    # 내부적으로 각 모듈의 매개변수는 requires_grad = True 일때, Variable 내에 저장되므로,\n",
    "    # 이 호출은 모든 모델의 모든 학습 가능한 매개변수의 변화도를 계산하게 됩니다.\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # 경사하강법(Gradient Descent) 를 사용하여 가중치를 갱신합니다. \n",
    "    # 각 매개변수는 Variable이므로 이전에 햇던 것과 같이 데이터와 변화도에 접근할 수 있습니다.\n",
    "    \n",
    "    for param in model.parameters() :\n",
    "        param.data -= learning_rate * param.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch : optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금 까지는 학습 가능한 매개변수를 갖는 Variable의 .data멤버를 직접 조작하여 모델의 가중치를 갱신했습니다. 이는 확률적 경사 하강법(SGD)과 같은 간단한 최적화 알괴즘에는 크게 부담이 되지는 않지만, 실제로 신경망을 학습할 떄는 주로 AdaGrad, RMSProp, Adam등과 같은 좀더 정교한 Optimizer를 사용합니다.  \n",
    "\n",
    "Pytorch의 optim 패키지는 최적화 알고리즘의 아이디어를 추상화 하고 일반적으로 사용하는 최적화 알고리즘의 구현체 (implementation)을 제공합니다.  \n",
    "\n",
    "이 예제에서는 앞에서와 같이 nn패키지를 사용하여 모델을 정의하지만 optim 패키지가 제공하는 adam알고리즘을 이용하여 모델을 최적화 합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(705.6899)\n",
      "1 tensor(688.6937)\n",
      "2 tensor(672.2026)\n",
      "3 tensor(656.2902)\n",
      "4 tensor(640.9213)\n",
      "5 tensor(626.0710)\n",
      "6 tensor(611.6686)\n",
      "7 tensor(597.7643)\n",
      "8 tensor(584.2377)\n",
      "9 tensor(571.1834)\n",
      "10 tensor(558.5079)\n",
      "11 tensor(546.2013)\n",
      "12 tensor(534.2230)\n",
      "13 tensor(522.5577)\n",
      "14 tensor(511.2400)\n",
      "15 tensor(500.2384)\n",
      "16 tensor(489.5085)\n",
      "17 tensor(479.0529)\n",
      "18 tensor(468.9060)\n",
      "19 tensor(459.0239)\n",
      "20 tensor(449.4133)\n",
      "21 tensor(440.0491)\n",
      "22 tensor(430.9098)\n",
      "23 tensor(422.0637)\n",
      "24 tensor(413.4578)\n",
      "25 tensor(405.0930)\n",
      "26 tensor(396.9392)\n",
      "27 tensor(388.9657)\n",
      "28 tensor(381.1513)\n",
      "29 tensor(373.4888)\n",
      "30 tensor(365.9742)\n",
      "31 tensor(358.6213)\n",
      "32 tensor(351.4160)\n",
      "33 tensor(344.3523)\n",
      "34 tensor(337.3904)\n",
      "35 tensor(330.5706)\n",
      "36 tensor(323.8667)\n",
      "37 tensor(317.2492)\n",
      "38 tensor(310.7351)\n",
      "39 tensor(304.3399)\n",
      "40 tensor(298.0602)\n",
      "41 tensor(291.8943)\n",
      "42 tensor(285.8506)\n",
      "43 tensor(279.9000)\n",
      "44 tensor(274.0649)\n",
      "45 tensor(268.3406)\n",
      "46 tensor(262.7173)\n",
      "47 tensor(257.2001)\n",
      "48 tensor(251.7941)\n",
      "49 tensor(246.4758)\n",
      "50 tensor(241.2376)\n",
      "51 tensor(236.1005)\n",
      "52 tensor(231.0654)\n",
      "53 tensor(226.0949)\n",
      "54 tensor(221.1904)\n",
      "55 tensor(216.3635)\n",
      "56 tensor(211.6302)\n",
      "57 tensor(206.9844)\n",
      "58 tensor(202.4150)\n",
      "59 tensor(197.9230)\n",
      "60 tensor(193.5052)\n",
      "61 tensor(189.1707)\n",
      "62 tensor(184.8963)\n",
      "63 tensor(180.6810)\n",
      "64 tensor(176.5320)\n",
      "65 tensor(172.4484)\n",
      "66 tensor(168.4405)\n",
      "67 tensor(164.5008)\n",
      "68 tensor(160.6292)\n",
      "69 tensor(156.8248)\n",
      "70 tensor(153.0853)\n",
      "71 tensor(149.4121)\n",
      "72 tensor(145.7993)\n",
      "73 tensor(142.2524)\n",
      "74 tensor(138.7699)\n",
      "75 tensor(135.3489)\n",
      "76 tensor(131.9891)\n",
      "77 tensor(128.6998)\n",
      "78 tensor(125.4686)\n",
      "79 tensor(122.2980)\n",
      "80 tensor(119.1891)\n",
      "81 tensor(116.1398)\n",
      "82 tensor(113.1556)\n",
      "83 tensor(110.2291)\n",
      "84 tensor(107.3491)\n",
      "85 tensor(104.5273)\n",
      "86 tensor(101.7608)\n",
      "87 tensor(99.0524)\n",
      "88 tensor(96.3969)\n",
      "89 tensor(93.8022)\n",
      "90 tensor(91.2611)\n",
      "91 tensor(88.7687)\n",
      "92 tensor(86.3247)\n",
      "93 tensor(83.9310)\n",
      "94 tensor(81.5877)\n",
      "95 tensor(79.2942)\n",
      "96 tensor(77.0484)\n",
      "97 tensor(74.8520)\n",
      "98 tensor(72.7026)\n",
      "99 tensor(70.6039)\n",
      "100 tensor(68.5495)\n",
      "101 tensor(66.5401)\n",
      "102 tensor(64.5818)\n",
      "103 tensor(62.6667)\n",
      "104 tensor(60.7989)\n",
      "105 tensor(58.9755)\n",
      "106 tensor(57.1997)\n",
      "107 tensor(55.4649)\n",
      "108 tensor(53.7740)\n",
      "109 tensor(52.1242)\n",
      "110 tensor(50.5162)\n",
      "111 tensor(48.9467)\n",
      "112 tensor(47.4177)\n",
      "113 tensor(45.9266)\n",
      "114 tensor(44.4756)\n",
      "115 tensor(43.0591)\n",
      "116 tensor(41.6823)\n",
      "117 tensor(40.3420)\n",
      "118 tensor(39.0370)\n",
      "119 tensor(37.7670)\n",
      "120 tensor(36.5318)\n",
      "121 tensor(35.3293)\n",
      "122 tensor(34.1595)\n",
      "123 tensor(33.0216)\n",
      "124 tensor(31.9179)\n",
      "125 tensor(30.8450)\n",
      "126 tensor(29.8021)\n",
      "127 tensor(28.7904)\n",
      "128 tensor(27.8073)\n",
      "129 tensor(26.8504)\n",
      "130 tensor(25.9217)\n",
      "131 tensor(25.0200)\n",
      "132 tensor(24.1449)\n",
      "133 tensor(23.2967)\n",
      "134 tensor(22.4735)\n",
      "135 tensor(21.6755)\n",
      "136 tensor(20.9020)\n",
      "137 tensor(20.1514)\n",
      "138 tensor(19.4261)\n",
      "139 tensor(18.7239)\n",
      "140 tensor(18.0442)\n",
      "141 tensor(17.3857)\n",
      "142 tensor(16.7496)\n",
      "143 tensor(16.1335)\n",
      "144 tensor(15.5385)\n",
      "145 tensor(14.9629)\n",
      "146 tensor(14.4075)\n",
      "147 tensor(13.8699)\n",
      "148 tensor(13.3510)\n",
      "149 tensor(12.8493)\n",
      "150 tensor(12.3647)\n",
      "151 tensor(11.8963)\n",
      "152 tensor(11.4447)\n",
      "153 tensor(11.0085)\n",
      "154 tensor(10.5874)\n",
      "155 tensor(10.1812)\n",
      "156 tensor(9.7892)\n",
      "157 tensor(9.4111)\n",
      "158 tensor(9.0466)\n",
      "159 tensor(8.6951)\n",
      "160 tensor(8.3562)\n",
      "161 tensor(8.0295)\n",
      "162 tensor(7.7145)\n",
      "163 tensor(7.4106)\n",
      "164 tensor(7.1180)\n",
      "165 tensor(6.8355)\n",
      "166 tensor(6.5632)\n",
      "167 tensor(6.3005)\n",
      "168 tensor(6.0471)\n",
      "169 tensor(5.8033)\n",
      "170 tensor(5.5681)\n",
      "171 tensor(5.3417)\n",
      "172 tensor(5.1236)\n",
      "173 tensor(4.9138)\n",
      "174 tensor(4.7118)\n",
      "175 tensor(4.5173)\n",
      "176 tensor(4.3304)\n",
      "177 tensor(4.1503)\n",
      "178 tensor(3.9774)\n",
      "179 tensor(3.8111)\n",
      "180 tensor(3.6512)\n",
      "181 tensor(3.4974)\n",
      "182 tensor(3.3496)\n",
      "183 tensor(3.2079)\n",
      "184 tensor(3.0716)\n",
      "185 tensor(2.9407)\n",
      "186 tensor(2.8151)\n",
      "187 tensor(2.6946)\n",
      "188 tensor(2.5787)\n",
      "189 tensor(2.4676)\n",
      "190 tensor(2.3609)\n",
      "191 tensor(2.2586)\n",
      "192 tensor(2.1604)\n",
      "193 tensor(2.0663)\n",
      "194 tensor(1.9760)\n",
      "195 tensor(1.8894)\n",
      "196 tensor(1.8064)\n",
      "197 tensor(1.7269)\n",
      "198 tensor(1.6506)\n",
      "199 tensor(1.5775)\n",
      "200 tensor(1.5074)\n",
      "201 tensor(1.4403)\n",
      "202 tensor(1.3761)\n",
      "203 tensor(1.3145)\n",
      "204 tensor(1.2555)\n",
      "205 tensor(1.1991)\n",
      "206 tensor(1.1450)\n",
      "207 tensor(1.0933)\n",
      "208 tensor(1.0438)\n",
      "209 tensor(0.9963)\n",
      "210 tensor(0.9510)\n",
      "211 tensor(0.9077)\n",
      "212 tensor(0.8662)\n",
      "213 tensor(0.8268)\n",
      "214 tensor(0.7890)\n",
      "215 tensor(0.7529)\n",
      "216 tensor(0.7184)\n",
      "217 tensor(0.6853)\n",
      "218 tensor(0.6538)\n",
      "219 tensor(0.6236)\n",
      "220 tensor(0.5949)\n",
      "221 tensor(0.5675)\n",
      "222 tensor(0.5414)\n",
      "223 tensor(0.5164)\n",
      "224 tensor(0.4925)\n",
      "225 tensor(0.4697)\n",
      "226 tensor(0.4478)\n",
      "227 tensor(0.4270)\n",
      "228 tensor(0.4071)\n",
      "229 tensor(0.3881)\n",
      "230 tensor(0.3699)\n",
      "231 tensor(0.3526)\n",
      "232 tensor(0.3360)\n",
      "233 tensor(0.3202)\n",
      "234 tensor(0.3051)\n",
      "235 tensor(0.2908)\n",
      "236 tensor(0.2770)\n",
      "237 tensor(0.2639)\n",
      "238 tensor(0.2514)\n",
      "239 tensor(0.2395)\n",
      "240 tensor(0.2281)\n",
      "241 tensor(0.2172)\n",
      "242 tensor(0.2068)\n",
      "243 tensor(0.1969)\n",
      "244 tensor(0.1874)\n",
      "245 tensor(0.1784)\n",
      "246 tensor(0.1698)\n",
      "247 tensor(0.1616)\n",
      "248 tensor(0.1538)\n",
      "249 tensor(0.1464)\n",
      "250 tensor(0.1392)\n",
      "251 tensor(0.1325)\n",
      "252 tensor(0.1260)\n",
      "253 tensor(0.1199)\n",
      "254 tensor(0.1140)\n",
      "255 tensor(0.1084)\n",
      "256 tensor(0.1031)\n",
      "257 tensor(0.0980)\n",
      "258 tensor(0.0932)\n",
      "259 tensor(0.0885)\n",
      "260 tensor(0.0842)\n",
      "261 tensor(0.0800)\n",
      "262 tensor(0.0760)\n",
      "263 tensor(0.0722)\n",
      "264 tensor(0.0686)\n",
      "265 tensor(0.0652)\n",
      "266 tensor(0.0619)\n",
      "267 tensor(0.0588)\n",
      "268 tensor(0.0558)\n",
      "269 tensor(0.0530)\n",
      "270 tensor(0.0503)\n",
      "271 tensor(0.0478)\n",
      "272 tensor(0.0453)\n",
      "273 tensor(0.0430)\n",
      "274 tensor(0.0408)\n",
      "275 tensor(0.0387)\n",
      "276 tensor(0.0367)\n",
      "277 tensor(0.0348)\n",
      "278 tensor(0.0330)\n",
      "279 tensor(0.0313)\n",
      "280 tensor(0.0297)\n",
      "281 tensor(0.0282)\n",
      "282 tensor(0.0267)\n",
      "283 tensor(0.0253)\n",
      "284 tensor(0.0240)\n",
      "285 tensor(0.0227)\n",
      "286 tensor(0.0215)\n",
      "287 tensor(0.0204)\n",
      "288 tensor(0.0193)\n",
      "289 tensor(0.0183)\n",
      "290 tensor(0.0173)\n",
      "291 tensor(0.0164)\n",
      "292 tensor(0.0156)\n",
      "293 tensor(0.0147)\n",
      "294 tensor(0.0139)\n",
      "295 tensor(0.0132)\n",
      "296 tensor(0.0125)\n",
      "297 tensor(0.0118)\n",
      "298 tensor(0.0112)\n",
      "299 tensor(0.0106)\n",
      "300 tensor(0.0100)\n",
      "301 tensor(0.0095)\n",
      "302 tensor(0.0090)\n",
      "303 tensor(0.0085)\n",
      "304 tensor(0.0080)\n",
      "305 tensor(0.0076)\n",
      "306 tensor(0.0072)\n",
      "307 tensor(0.0068)\n",
      "308 tensor(0.0064)\n",
      "309 tensor(0.0061)\n",
      "310 tensor(0.0057)\n",
      "311 tensor(0.0054)\n",
      "312 tensor(0.0051)\n",
      "313 tensor(0.0049)\n",
      "314 tensor(0.0046)\n",
      "315 tensor(0.0043)\n",
      "316 tensor(0.0041)\n",
      "317 tensor(0.0039)\n",
      "318 tensor(0.0037)\n",
      "319 tensor(0.0035)\n",
      "320 tensor(0.0033)\n",
      "321 tensor(0.0031)\n",
      "322 tensor(0.0029)\n",
      "323 tensor(0.0027)\n",
      "324 tensor(0.0026)\n",
      "325 tensor(0.0024)\n",
      "326 tensor(0.0023)\n",
      "327 tensor(0.0022)\n",
      "328 tensor(0.0021)\n",
      "329 tensor(0.0019)\n",
      "330 tensor(0.0018)\n",
      "331 tensor(0.0017)\n",
      "332 tensor(0.0016)\n",
      "333 tensor(0.0015)\n",
      "334 tensor(0.0015)\n",
      "335 tensor(0.0014)\n",
      "336 tensor(0.0013)\n",
      "337 tensor(0.0012)\n",
      "338 tensor(0.0011)\n",
      "339 tensor(0.0011)\n",
      "340 tensor(0.0010)\n",
      "341 tensor(0.0010)\n",
      "342 tensor(0.0009)\n",
      "343 tensor(0.0009)\n",
      "344 tensor(0.0008)\n",
      "345 tensor(0.0008)\n",
      "346 tensor(0.0007)\n",
      "347 tensor(0.0007)\n",
      "348 tensor(0.0006)\n",
      "349 tensor(0.0006)\n",
      "350 tensor(0.0006)\n",
      "351 tensor(0.0005)\n",
      "352 tensor(0.0005)\n",
      "353 tensor(0.0005)\n",
      "354 tensor(0.0004)\n",
      "355 tensor(0.0004)\n",
      "356 tensor(0.0004)\n",
      "357 tensor(0.0004)\n",
      "358 tensor(0.0003)\n",
      "359 tensor(0.0003)\n",
      "360 tensor(0.0003)\n",
      "361 tensor(0.0003)\n",
      "362 tensor(0.0003)\n",
      "363 tensor(0.0003)\n",
      "364 tensor(0.0002)\n",
      "365 tensor(0.0002)\n",
      "366 tensor(0.0002)\n",
      "367 tensor(0.0002)\n",
      "368 tensor(0.0002)\n",
      "369 tensor(0.0002)\n",
      "370 tensor(0.0002)\n",
      "371 tensor(0.0002)\n",
      "372 tensor(0.0001)\n",
      "373 tensor(0.0001)\n",
      "374 tensor(0.0001)\n",
      "375 tensor(0.0001)\n",
      "376 tensor(0.0001)\n",
      "377 tensor(0.0001)\n",
      "378 tensor(0.0001)\n",
      "379 tensor(9.5332e-05)\n",
      "380 tensor(8.9544e-05)\n",
      "381 tensor(8.4100e-05)\n",
      "382 tensor(7.8975e-05)\n",
      "383 tensor(7.4162e-05)\n",
      "384 tensor(6.9630e-05)\n",
      "385 tensor(6.5369e-05)\n",
      "386 tensor(6.1368e-05)\n",
      "387 tensor(5.7599e-05)\n",
      "388 tensor(5.4064e-05)\n",
      "389 tensor(5.0738e-05)\n",
      "390 tensor(4.7610e-05)\n",
      "391 tensor(4.4675e-05)\n",
      "392 tensor(4.1916e-05)\n",
      "393 tensor(3.9323e-05)\n",
      "394 tensor(3.6887e-05)\n",
      "395 tensor(3.4599e-05)\n",
      "396 tensor(3.2447e-05)\n",
      "397 tensor(3.0433e-05)\n",
      "398 tensor(2.8538e-05)\n",
      "399 tensor(2.6757e-05)\n",
      "400 tensor(2.5084e-05)\n",
      "401 tensor(2.3517e-05)\n",
      "402 tensor(2.2043e-05)\n",
      "403 tensor(2.0661e-05)\n",
      "404 tensor(1.9364e-05)\n",
      "405 tensor(1.8146e-05)\n",
      "406 tensor(1.7003e-05)\n",
      "407 tensor(1.5932e-05)\n",
      "408 tensor(1.4926e-05)\n",
      "409 tensor(1.3980e-05)\n",
      "410 tensor(1.3097e-05)\n",
      "411 tensor(1.2265e-05)\n",
      "412 tensor(1.1486e-05)\n",
      "413 tensor(1.0756e-05)\n",
      "414 tensor(1.0073e-05)\n",
      "415 tensor(9.4291e-06)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416 tensor(8.8287e-06)\n",
      "417 tensor(8.2639e-06)\n",
      "418 tensor(7.7342e-06)\n",
      "419 tensor(7.2397e-06)\n",
      "420 tensor(6.7745e-06)\n",
      "421 tensor(6.3387e-06)\n",
      "422 tensor(5.9330e-06)\n",
      "423 tensor(5.5508e-06)\n",
      "424 tensor(5.1921e-06)\n",
      "425 tensor(4.8576e-06)\n",
      "426 tensor(4.5431e-06)\n",
      "427 tensor(4.2508e-06)\n",
      "428 tensor(3.9742e-06)\n",
      "429 tensor(3.7169e-06)\n",
      "430 tensor(3.4766e-06)\n",
      "431 tensor(3.2499e-06)\n",
      "432 tensor(3.0384e-06)\n",
      "433 tensor(2.8398e-06)\n",
      "434 tensor(2.6555e-06)\n",
      "435 tensor(2.4826e-06)\n",
      "436 tensor(2.3196e-06)\n",
      "437 tensor(2.1683e-06)\n",
      "438 tensor(2.0261e-06)\n",
      "439 tensor(1.8927e-06)\n",
      "440 tensor(1.7696e-06)\n",
      "441 tensor(1.6526e-06)\n",
      "442 tensor(1.5439e-06)\n",
      "443 tensor(1.4426e-06)\n",
      "444 tensor(1.3474e-06)\n",
      "445 tensor(1.2584e-06)\n",
      "446 tensor(1.1753e-06)\n",
      "447 tensor(1.0978e-06)\n",
      "448 tensor(1.0251e-06)\n",
      "449 tensor(9.5764e-07)\n",
      "450 tensor(8.9379e-07)\n",
      "451 tensor(8.3461e-07)\n",
      "452 tensor(7.7864e-07)\n",
      "453 tensor(7.2722e-07)\n",
      "454 tensor(6.7877e-07)\n",
      "455 tensor(6.3348e-07)\n",
      "456 tensor(5.9139e-07)\n",
      "457 tensor(5.5197e-07)\n",
      "458 tensor(5.1516e-07)\n",
      "459 tensor(4.8087e-07)\n",
      "460 tensor(4.4893e-07)\n",
      "461 tensor(4.1885e-07)\n",
      "462 tensor(3.9103e-07)\n",
      "463 tensor(3.6495e-07)\n",
      "464 tensor(3.4053e-07)\n",
      "465 tensor(3.1750e-07)\n",
      "466 tensor(2.9642e-07)\n",
      "467 tensor(2.7647e-07)\n",
      "468 tensor(2.5784e-07)\n",
      "469 tensor(2.4055e-07)\n",
      "470 tensor(2.2431e-07)\n",
      "471 tensor(2.0934e-07)\n",
      "472 tensor(1.9530e-07)\n",
      "473 tensor(1.8212e-07)\n",
      "474 tensor(1.6959e-07)\n",
      "475 tensor(1.5846e-07)\n",
      "476 tensor(1.4764e-07)\n",
      "477 tensor(1.3764e-07)\n",
      "478 tensor(1.2841e-07)\n",
      "479 tensor(1.1971e-07)\n",
      "480 tensor(1.1164e-07)\n",
      "481 tensor(1.0408e-07)\n",
      "482 tensor(9.7098e-08)\n",
      "483 tensor(9.0570e-08)\n",
      "484 tensor(8.4498e-08)\n",
      "485 tensor(7.8763e-08)\n",
      "486 tensor(7.3450e-08)\n",
      "487 tensor(6.8562e-08)\n",
      "488 tensor(6.3917e-08)\n",
      "489 tensor(5.9625e-08)\n",
      "490 tensor(5.5580e-08)\n",
      "491 tensor(5.1893e-08)\n",
      "492 tensor(4.8324e-08)\n",
      "493 tensor(4.5127e-08)\n",
      "494 tensor(4.2098e-08)\n",
      "495 tensor(3.9222e-08)\n",
      "496 tensor(3.6609e-08)\n",
      "497 tensor(3.4126e-08)\n",
      "498 tensor(3.1793e-08)\n",
      "499 tensor(2.9663e-08)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉 계층의 차원이며, D_out은 출력 차원입니다:\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성하고, Variable로\n",
    "# 감쌉니다.\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "## nn패키지를 사용하여 모델과 손실함수를 정의\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "# optim 패키지를 사용하여 모델의 가중치를 갱신할 Optimizer를 정의합니다.\n",
    "# 여기서는 Adam을 사용합니다. optim 패키지는 다른 다양한 최적화 알고리즘을 포함하고 있습니다.\n",
    "# Adam 생성자의 첫 인자는 갱신해야 하는 Variable을 Optimizer에 알려줍니다.\n",
    "\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(500) :\n",
    "    # 순전파 단계: 모델에 x를 전달하여 예상하는 y 값을 계산합니다.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.data)\n",
    "\n",
    "    # 역전파 단계 전에, Optimizer 객체를 사용하여 (모델의 학습 가능한 가중치인)\n",
    "    # 갱신할 Variable들에 대한 모든 변화도를 0으로 만듭니다. 이는 기본적으로,\n",
    "    # .backward()를 호출할 때마다 변화도가 버퍼(Buffer)에 (덮어쓰지 않고) 누적되기\n",
    "    # 때문입니다. 더 자세한 내용은 torch.autograd.backward에 대한 문서를 참조하세요.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 역전파 단계: 모델의 매개변수에 대한 손실의 변화도를 계산합니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimizer의 step 함수를 호출하면 매개변수가 갱신됩니다.\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch : 사용자 정의 nn 모듈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가끔은 기존 모듈의 순차적 구성보다 더 복잡한 모델을 구성해야 할 때가 있습니다; 이럴 떄는 nn.Module의 서브클래스로 새 모듈을 정의하고, 입력 Variable 을 받아 다른 모듈 또는 Variable의 autograd 연산을 사용하여 출력 Variable을 생성하는 foward를 정의합니다.  \n",
    "\n",
    "이 예제에서는 2-계층 신경망을 사용자 정의 Module의 서브 클래스로 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(773.8176)\n",
      "1 tensor(714.5993)\n",
      "2 tensor(663.2770)\n",
      "3 tensor(618.1769)\n",
      "4 tensor(578.0924)\n",
      "5 tensor(541.9462)\n",
      "6 tensor(508.9998)\n",
      "7 tensor(478.8720)\n",
      "8 tensor(451.3778)\n",
      "9 tensor(425.8236)\n",
      "10 tensor(401.9220)\n",
      "11 tensor(379.5479)\n",
      "12 tensor(358.5653)\n",
      "13 tensor(338.7312)\n",
      "14 tensor(319.9577)\n",
      "15 tensor(302.0431)\n",
      "16 tensor(285.0070)\n",
      "17 tensor(268.8488)\n",
      "18 tensor(253.5112)\n",
      "19 tensor(238.9655)\n",
      "20 tensor(225.1216)\n",
      "21 tensor(211.9780)\n",
      "22 tensor(199.5020)\n",
      "23 tensor(187.7109)\n",
      "24 tensor(176.5349)\n",
      "25 tensor(165.9437)\n",
      "26 tensor(155.8990)\n",
      "27 tensor(146.4039)\n",
      "28 tensor(137.4297)\n",
      "29 tensor(128.9574)\n",
      "30 tensor(120.9555)\n",
      "31 tensor(113.4236)\n",
      "32 tensor(106.3435)\n",
      "33 tensor(99.6567)\n",
      "34 tensor(93.3956)\n",
      "35 tensor(87.4652)\n",
      "36 tensor(81.9112)\n",
      "37 tensor(76.7008)\n",
      "38 tensor(71.8178)\n",
      "39 tensor(67.2404)\n",
      "40 tensor(62.9282)\n",
      "41 tensor(58.9027)\n",
      "42 tensor(55.1428)\n",
      "43 tensor(51.6382)\n",
      "44 tensor(48.3680)\n",
      "45 tensor(45.3114)\n",
      "46 tensor(42.4581)\n",
      "47 tensor(39.7938)\n",
      "48 tensor(37.3071)\n",
      "49 tensor(34.9872)\n",
      "50 tensor(32.8255)\n",
      "51 tensor(30.7990)\n",
      "52 tensor(28.9105)\n",
      "53 tensor(27.1492)\n",
      "54 tensor(25.4993)\n",
      "55 tensor(23.9615)\n",
      "56 tensor(22.5259)\n",
      "57 tensor(21.1850)\n",
      "58 tensor(19.9299)\n",
      "59 tensor(18.7579)\n",
      "60 tensor(17.6577)\n",
      "61 tensor(16.6245)\n",
      "62 tensor(15.6589)\n",
      "63 tensor(14.7558)\n",
      "64 tensor(13.9115)\n",
      "65 tensor(13.1204)\n",
      "66 tensor(12.3789)\n",
      "67 tensor(11.6846)\n",
      "68 tensor(11.0334)\n",
      "69 tensor(10.4226)\n",
      "70 tensor(9.8490)\n",
      "71 tensor(9.3108)\n",
      "72 tensor(8.8045)\n",
      "73 tensor(8.3284)\n",
      "74 tensor(7.8809)\n",
      "75 tensor(7.4608)\n",
      "76 tensor(7.0652)\n",
      "77 tensor(6.6925)\n",
      "78 tensor(6.3416)\n",
      "79 tensor(6.0110)\n",
      "80 tensor(5.6995)\n",
      "81 tensor(5.4056)\n",
      "82 tensor(5.1283)\n",
      "83 tensor(4.8666)\n",
      "84 tensor(4.6196)\n",
      "85 tensor(4.3865)\n",
      "86 tensor(4.1659)\n",
      "87 tensor(3.9573)\n",
      "88 tensor(3.7599)\n",
      "89 tensor(3.5733)\n",
      "90 tensor(3.3967)\n",
      "91 tensor(3.2293)\n",
      "92 tensor(3.0709)\n",
      "93 tensor(2.9210)\n",
      "94 tensor(2.7792)\n",
      "95 tensor(2.6448)\n",
      "96 tensor(2.5174)\n",
      "97 tensor(2.3964)\n",
      "98 tensor(2.2817)\n",
      "99 tensor(2.1728)\n",
      "100 tensor(2.0696)\n",
      "101 tensor(1.9715)\n",
      "102 tensor(1.8784)\n",
      "103 tensor(1.7900)\n",
      "104 tensor(1.7059)\n",
      "105 tensor(1.6259)\n",
      "106 tensor(1.5500)\n",
      "107 tensor(1.4779)\n",
      "108 tensor(1.4096)\n",
      "109 tensor(1.3446)\n",
      "110 tensor(1.2827)\n",
      "111 tensor(1.2239)\n",
      "112 tensor(1.1680)\n",
      "113 tensor(1.1147)\n",
      "114 tensor(1.0640)\n",
      "115 tensor(1.0158)\n",
      "116 tensor(0.9698)\n",
      "117 tensor(0.9261)\n",
      "118 tensor(0.8844)\n",
      "119 tensor(0.8448)\n",
      "120 tensor(0.8070)\n",
      "121 tensor(0.7710)\n",
      "122 tensor(0.7366)\n",
      "123 tensor(0.7039)\n",
      "124 tensor(0.6727)\n",
      "125 tensor(0.6430)\n",
      "126 tensor(0.6146)\n",
      "127 tensor(0.5876)\n",
      "128 tensor(0.5617)\n",
      "129 tensor(0.5371)\n",
      "130 tensor(0.5137)\n",
      "131 tensor(0.4913)\n",
      "132 tensor(0.4699)\n",
      "133 tensor(0.4495)\n",
      "134 tensor(0.4300)\n",
      "135 tensor(0.4114)\n",
      "136 tensor(0.3936)\n",
      "137 tensor(0.3767)\n",
      "138 tensor(0.3605)\n",
      "139 tensor(0.3450)\n",
      "140 tensor(0.3303)\n",
      "141 tensor(0.3162)\n",
      "142 tensor(0.3028)\n",
      "143 tensor(0.2899)\n",
      "144 tensor(0.2777)\n",
      "145 tensor(0.2659)\n",
      "146 tensor(0.2547)\n",
      "147 tensor(0.2440)\n",
      "148 tensor(0.2338)\n",
      "149 tensor(0.2240)\n",
      "150 tensor(0.2146)\n",
      "151 tensor(0.2057)\n",
      "152 tensor(0.1971)\n",
      "153 tensor(0.1890)\n",
      "154 tensor(0.1812)\n",
      "155 tensor(0.1737)\n",
      "156 tensor(0.1665)\n",
      "157 tensor(0.1597)\n",
      "158 tensor(0.1532)\n",
      "159 tensor(0.1469)\n",
      "160 tensor(0.1409)\n",
      "161 tensor(0.1351)\n",
      "162 tensor(0.1296)\n",
      "163 tensor(0.1244)\n",
      "164 tensor(0.1193)\n",
      "165 tensor(0.1145)\n",
      "166 tensor(0.1099)\n",
      "167 tensor(0.1054)\n",
      "168 tensor(0.1012)\n",
      "169 tensor(0.0971)\n",
      "170 tensor(0.0933)\n",
      "171 tensor(0.0895)\n",
      "172 tensor(0.0860)\n",
      "173 tensor(0.0825)\n",
      "174 tensor(0.0793)\n",
      "175 tensor(0.0761)\n",
      "176 tensor(0.0731)\n",
      "177 tensor(0.0702)\n",
      "178 tensor(0.0674)\n",
      "179 tensor(0.0648)\n",
      "180 tensor(0.0622)\n",
      "181 tensor(0.0598)\n",
      "182 tensor(0.0575)\n",
      "183 tensor(0.0552)\n",
      "184 tensor(0.0530)\n",
      "185 tensor(0.0510)\n",
      "186 tensor(0.0490)\n",
      "187 tensor(0.0471)\n",
      "188 tensor(0.0453)\n",
      "189 tensor(0.0435)\n",
      "190 tensor(0.0418)\n",
      "191 tensor(0.0402)\n",
      "192 tensor(0.0386)\n",
      "193 tensor(0.0372)\n",
      "194 tensor(0.0357)\n",
      "195 tensor(0.0344)\n",
      "196 tensor(0.0330)\n",
      "197 tensor(0.0318)\n",
      "198 tensor(0.0306)\n",
      "199 tensor(0.0294)\n",
      "200 tensor(0.0283)\n",
      "201 tensor(0.0272)\n",
      "202 tensor(0.0262)\n",
      "203 tensor(0.0252)\n",
      "204 tensor(0.0242)\n",
      "205 tensor(0.0233)\n",
      "206 tensor(0.0224)\n",
      "207 tensor(0.0216)\n",
      "208 tensor(0.0208)\n",
      "209 tensor(0.0200)\n",
      "210 tensor(0.0192)\n",
      "211 tensor(0.0185)\n",
      "212 tensor(0.0178)\n",
      "213 tensor(0.0171)\n",
      "214 tensor(0.0165)\n",
      "215 tensor(0.0159)\n",
      "216 tensor(0.0153)\n",
      "217 tensor(0.0147)\n",
      "218 tensor(0.0142)\n",
      "219 tensor(0.0136)\n",
      "220 tensor(0.0131)\n",
      "221 tensor(0.0127)\n",
      "222 tensor(0.0122)\n",
      "223 tensor(0.0117)\n",
      "224 tensor(0.0113)\n",
      "225 tensor(0.0109)\n",
      "226 tensor(0.0105)\n",
      "227 tensor(0.0101)\n",
      "228 tensor(0.0097)\n",
      "229 tensor(0.0094)\n",
      "230 tensor(0.0090)\n",
      "231 tensor(0.0087)\n",
      "232 tensor(0.0084)\n",
      "233 tensor(0.0081)\n",
      "234 tensor(0.0078)\n",
      "235 tensor(0.0075)\n",
      "236 tensor(0.0072)\n",
      "237 tensor(0.0070)\n",
      "238 tensor(0.0067)\n",
      "239 tensor(0.0065)\n",
      "240 tensor(0.0062)\n",
      "241 tensor(0.0060)\n",
      "242 tensor(0.0058)\n",
      "243 tensor(0.0056)\n",
      "244 tensor(0.0054)\n",
      "245 tensor(0.0052)\n",
      "246 tensor(0.0050)\n",
      "247 tensor(0.0048)\n",
      "248 tensor(0.0047)\n",
      "249 tensor(0.0045)\n",
      "250 tensor(0.0043)\n",
      "251 tensor(0.0042)\n",
      "252 tensor(0.0040)\n",
      "253 tensor(0.0039)\n",
      "254 tensor(0.0037)\n",
      "255 tensor(0.0036)\n",
      "256 tensor(0.0035)\n",
      "257 tensor(0.0034)\n",
      "258 tensor(0.0032)\n",
      "259 tensor(0.0031)\n",
      "260 tensor(0.0030)\n",
      "261 tensor(0.0029)\n",
      "262 tensor(0.0028)\n",
      "263 tensor(0.0027)\n",
      "264 tensor(0.0026)\n",
      "265 tensor(0.0025)\n",
      "266 tensor(0.0024)\n",
      "267 tensor(0.0023)\n",
      "268 tensor(0.0023)\n",
      "269 tensor(0.0022)\n",
      "270 tensor(0.0021)\n",
      "271 tensor(0.0020)\n",
      "272 tensor(0.0020)\n",
      "273 tensor(0.0019)\n",
      "274 tensor(0.0018)\n",
      "275 tensor(0.0018)\n",
      "276 tensor(0.0017)\n",
      "277 tensor(0.0016)\n",
      "278 tensor(0.0016)\n",
      "279 tensor(0.0015)\n",
      "280 tensor(0.0015)\n",
      "281 tensor(0.0014)\n",
      "282 tensor(0.0014)\n",
      "283 tensor(0.0013)\n",
      "284 tensor(0.0013)\n",
      "285 tensor(0.0012)\n",
      "286 tensor(0.0012)\n",
      "287 tensor(0.0012)\n",
      "288 tensor(0.0011)\n",
      "289 tensor(0.0011)\n",
      "290 tensor(0.0010)\n",
      "291 tensor(0.0010)\n",
      "292 tensor(0.0010)\n",
      "293 tensor(0.0009)\n",
      "294 tensor(0.0009)\n",
      "295 tensor(0.0009)\n",
      "296 tensor(0.0008)\n",
      "297 tensor(0.0008)\n",
      "298 tensor(0.0008)\n",
      "299 tensor(0.0008)\n",
      "300 tensor(0.0007)\n",
      "301 tensor(0.0007)\n",
      "302 tensor(0.0007)\n",
      "303 tensor(0.0007)\n",
      "304 tensor(0.0006)\n",
      "305 tensor(0.0006)\n",
      "306 tensor(0.0006)\n",
      "307 tensor(0.0006)\n",
      "308 tensor(0.0006)\n",
      "309 tensor(0.0005)\n",
      "310 tensor(0.0005)\n",
      "311 tensor(0.0005)\n",
      "312 tensor(0.0005)\n",
      "313 tensor(0.0005)\n",
      "314 tensor(0.0005)\n",
      "315 tensor(0.0004)\n",
      "316 tensor(0.0004)\n",
      "317 tensor(0.0004)\n",
      "318 tensor(0.0004)\n",
      "319 tensor(0.0004)\n",
      "320 tensor(0.0004)\n",
      "321 tensor(0.0004)\n",
      "322 tensor(0.0003)\n",
      "323 tensor(0.0003)\n",
      "324 tensor(0.0003)\n",
      "325 tensor(0.0003)\n",
      "326 tensor(0.0003)\n",
      "327 tensor(0.0003)\n",
      "328 tensor(0.0003)\n",
      "329 tensor(0.0003)\n",
      "330 tensor(0.0003)\n",
      "331 tensor(0.0003)\n",
      "332 tensor(0.0003)\n",
      "333 tensor(0.0002)\n",
      "334 tensor(0.0002)\n",
      "335 tensor(0.0002)\n",
      "336 tensor(0.0002)\n",
      "337 tensor(0.0002)\n",
      "338 tensor(0.0002)\n",
      "339 tensor(0.0002)\n",
      "340 tensor(0.0002)\n",
      "341 tensor(0.0002)\n",
      "342 tensor(0.0002)\n",
      "343 tensor(0.0002)\n",
      "344 tensor(0.0002)\n",
      "345 tensor(0.0002)\n",
      "346 tensor(0.0002)\n",
      "347 tensor(0.0002)\n",
      "348 tensor(0.0001)\n",
      "349 tensor(0.0001)\n",
      "350 tensor(0.0001)\n",
      "351 tensor(0.0001)\n",
      "352 tensor(0.0001)\n",
      "353 tensor(0.0001)\n",
      "354 tensor(0.0001)\n",
      "355 tensor(0.0001)\n",
      "356 tensor(0.0001)\n",
      "357 tensor(0.0001)\n",
      "358 tensor(0.0001)\n",
      "359 tensor(0.0001)\n",
      "360 tensor(9.9276e-05)\n",
      "361 tensor(9.6085e-05)\n",
      "362 tensor(9.2999e-05)\n",
      "363 tensor(9.0015e-05)\n",
      "364 tensor(8.7131e-05)\n",
      "365 tensor(8.4339e-05)\n",
      "366 tensor(8.1640e-05)\n",
      "367 tensor(7.9025e-05)\n",
      "368 tensor(7.6496e-05)\n",
      "369 tensor(7.4056e-05)\n",
      "370 tensor(7.1691e-05)\n",
      "371 tensor(6.9404e-05)\n",
      "372 tensor(6.7197e-05)\n",
      "373 tensor(6.5053e-05)\n",
      "374 tensor(6.2980e-05)\n",
      "375 tensor(6.0979e-05)\n",
      "376 tensor(5.9037e-05)\n",
      "377 tensor(5.7160e-05)\n",
      "378 tensor(5.5345e-05)\n",
      "379 tensor(5.3590e-05)\n",
      "380 tensor(5.1890e-05)\n",
      "381 tensor(5.0244e-05)\n",
      "382 tensor(4.8650e-05)\n",
      "383 tensor(4.7113e-05)\n",
      "384 tensor(4.5620e-05)\n",
      "385 tensor(4.4180e-05)\n",
      "386 tensor(4.2783e-05)\n",
      "387 tensor(4.1429e-05)\n",
      "388 tensor(4.0122e-05)\n",
      "389 tensor(3.8856e-05)\n",
      "390 tensor(3.7631e-05)\n",
      "391 tensor(3.6444e-05)\n",
      "392 tensor(3.5296e-05)\n",
      "393 tensor(3.4186e-05)\n",
      "394 tensor(3.3115e-05)\n",
      "395 tensor(3.2071e-05)\n",
      "396 tensor(3.1061e-05)\n",
      "397 tensor(3.0092e-05)\n",
      "398 tensor(2.9142e-05)\n",
      "399 tensor(2.8232e-05)\n",
      "400 tensor(2.7345e-05)\n",
      "401 tensor(2.6487e-05)\n",
      "402 tensor(2.5658e-05)\n",
      "403 tensor(2.4855e-05)\n",
      "404 tensor(2.4078e-05)\n",
      "405 tensor(2.3326e-05)\n",
      "406 tensor(2.2596e-05)\n",
      "407 tensor(2.1890e-05)\n",
      "408 tensor(2.1207e-05)\n",
      "409 tensor(2.0545e-05)\n",
      "410 tensor(1.9906e-05)\n",
      "411 tensor(1.9287e-05)\n",
      "412 tensor(1.8684e-05)\n",
      "413 tensor(1.8102e-05)\n",
      "414 tensor(1.7539e-05)\n",
      "415 tensor(1.6993e-05)\n",
      "416 tensor(1.6466e-05)\n",
      "417 tensor(1.5954e-05)\n",
      "418 tensor(1.5460e-05)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419 tensor(1.4978e-05)\n",
      "420 tensor(1.4513e-05)\n",
      "421 tensor(1.4064e-05)\n",
      "422 tensor(1.3627e-05)\n",
      "423 tensor(1.3205e-05)\n",
      "424 tensor(1.2796e-05)\n",
      "425 tensor(1.2400e-05)\n",
      "426 tensor(1.2017e-05)\n",
      "427 tensor(1.1645e-05)\n",
      "428 tensor(1.1285e-05)\n",
      "429 tensor(1.0938e-05)\n",
      "430 tensor(1.0598e-05)\n",
      "431 tensor(1.0271e-05)\n",
      "432 tensor(9.9542e-06)\n",
      "433 tensor(9.6473e-06)\n",
      "434 tensor(9.3505e-06)\n",
      "435 tensor(9.0616e-06)\n",
      "436 tensor(8.7832e-06)\n",
      "437 tensor(8.5120e-06)\n",
      "438 tensor(8.2504e-06)\n",
      "439 tensor(7.9967e-06)\n",
      "440 tensor(7.7512e-06)\n",
      "441 tensor(7.5130e-06)\n",
      "442 tensor(7.2829e-06)\n",
      "443 tensor(7.0583e-06)\n",
      "444 tensor(6.8429e-06)\n",
      "445 tensor(6.6319e-06)\n",
      "446 tensor(6.4294e-06)\n",
      "447 tensor(6.2306e-06)\n",
      "448 tensor(6.0403e-06)\n",
      "449 tensor(5.8548e-06)\n",
      "450 tensor(5.6754e-06)\n",
      "451 tensor(5.5023e-06)\n",
      "452 tensor(5.3347e-06)\n",
      "453 tensor(5.1723e-06)\n",
      "454 tensor(5.0125e-06)\n",
      "455 tensor(4.8609e-06)\n",
      "456 tensor(4.7118e-06)\n",
      "457 tensor(4.5675e-06)\n",
      "458 tensor(4.4288e-06)\n",
      "459 tensor(4.2941e-06)\n",
      "460 tensor(4.1619e-06)\n",
      "461 tensor(4.0354e-06)\n",
      "462 tensor(3.9127e-06)\n",
      "463 tensor(3.7932e-06)\n",
      "464 tensor(3.6776e-06)\n",
      "465 tensor(3.5662e-06)\n",
      "466 tensor(3.4579e-06)\n",
      "467 tensor(3.3528e-06)\n",
      "468 tensor(3.2504e-06)\n",
      "469 tensor(3.1518e-06)\n",
      "470 tensor(3.0556e-06)\n",
      "471 tensor(2.9629e-06)\n",
      "472 tensor(2.8730e-06)\n",
      "473 tensor(2.7859e-06)\n",
      "474 tensor(2.7015e-06)\n",
      "475 tensor(2.6195e-06)\n",
      "476 tensor(2.5402e-06)\n",
      "477 tensor(2.4634e-06)\n",
      "478 tensor(2.3885e-06)\n",
      "479 tensor(2.3165e-06)\n",
      "480 tensor(2.2461e-06)\n",
      "481 tensor(2.1776e-06)\n",
      "482 tensor(2.1120e-06)\n",
      "483 tensor(2.0483e-06)\n",
      "484 tensor(1.9866e-06)\n",
      "485 tensor(1.9266e-06)\n",
      "486 tensor(1.8683e-06)\n",
      "487 tensor(1.8115e-06)\n",
      "488 tensor(1.7570e-06)\n",
      "489 tensor(1.7041e-06)\n",
      "490 tensor(1.6525e-06)\n",
      "491 tensor(1.6033e-06)\n",
      "492 tensor(1.5546e-06)\n",
      "493 tensor(1.5072e-06)\n",
      "494 tensor(1.4619e-06)\n",
      "495 tensor(1.4181e-06)\n",
      "496 tensor(1.3759e-06)\n",
      "497 tensor(1.3340e-06)\n",
      "498 tensor(1.2939e-06)\n",
      "499 tensor(1.2553e-06)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module) :\n",
    "    def __init__(self, D_in, H, D_out) :\n",
    "        \"\"\"\n",
    "        생성자에서 2개의 nn.Linear 모듈을 생성(Instantiate) 하고, 멤버 변수로 지정합니다.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        \"\"\"\n",
    "        순전파 함수에서는 입력 데이터의 Variable을 받아서 출력 데이터의 Variable을 반환해야 합니다.\n",
    "        Variable 상의 임의의 연산자뿐만 아니라 생성자에서 정의한 모듈을 사용할 수 있습니다.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "    \n",
    "# N은 배치 크기이며, D_in은 입력의 차원이다.\n",
    "# H는 은닉 계층의 차원이며, D_out은 출력 차원이다.\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성하고, Variable로\n",
    "# 감쌉니다.\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "# 앞에서 정의한 클래스를 생성(Instantiating)해서 모델을 구성!!합니다.\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# 손실함수와 Optimizer를 만듭니다. SGD 생성자에서 model.parameters()를 호출하면\n",
    "# 모델의 멤버인 2개의 nnLinear 모듈의 학습 가능한 매개변수들이 포함됩니다.\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "for t in range(500) :\n",
    "    # 순전파 단계 : 모델에 x를 전달 하여 예상하는 y값을 계산합니다.\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # 손실을 계산하고 출력합니다.\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.data)\n",
    "    \n",
    "    # 변화도를 0으로 만들고, 역전파 단계를 수행하고, 가중치를 갱신합니다.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch : 제어흐름 (Control Flow) + 가중치 공유( Weight Sharing) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 순전파 단계에서 많은 은닉 계층을 갖는 완전히 연결(Fully-connected)된 ReLU 신경망이 무작위로 1 ~ 4 사이의 숫자를 선택하고, 동일한 가중치를 여러 번 재사용하여 가장 안쪽(Innermost)에 있는 은닉 계층들을 계산합니다.\n",
    "\n",
    "이 모델에서는 반복문을 구현하기 위해 일반적인 Python 제어 흐름을 사용하고, 순전파 단계를 정의할 때 단지 동일한 모듈을 여러번 재사용함으로써 내부(innermost) 계층들 간의 가중치 공유를 구현할 수 있습니다.\n",
    "\n",
    "이 모델은 간단히 Module을 상속받는 서브클래스로 구현해보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
