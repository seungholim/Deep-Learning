{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Network\n",
    "\n",
    "- Convolution\n",
    "- MNIST\n",
    "- Pytorch Visdom\n",
    "- Pytorch Datasets & Custom Dataset\n",
    "- CIFAR-10\n",
    "- VGG & ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convolution\n",
    "\n",
    "- Convolution?\n",
    "- Neuron 과 Convolution\n",
    "- Pooling\n",
    "- etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1 Convolution 이란?\n",
    "\n",
    "- 이미지 위에서 stride 값 만큼 filter(kernel)을 이동시키면서 겹쳐지는 부분의 각 원소의 값을 곱해서 모두더한 값을 출력으로 하는 연산입니다. \n",
    "\n",
    "![](https://camo.githubusercontent.com/3309220c48ab22c9a5dfe7656c3f1639b6b1755d/68747470733a2f2f7777772e64726f70626f782e636f6d2f732f6e3134713930677a386138726278622f32645f636f6e766f6c7574696f6e2e706e673f7261773d31)\n",
    "\n",
    "위의 그림에선\n",
    "I = input\n",
    "K = filter \n",
    "I*K = output\n",
    "\n",
    "### 1-2 Stride and Padding\n",
    "\n",
    "- Stride : filter를 한번에 얼마나 이동할 것인가?\n",
    "- padding : zero-padding\n",
    "    \n",
    "### 1-3 Pytorch.nn.Conv2d\n",
    "torch.nn.Conv2d(in_channel,out_channel,kernel_size,stride=1,padding=0,dilation=1,group=1,bias=True)  \n",
    "![](img/20191215_201335.png)\n",
    "\n",
    "### 1-4 입력의 형태\n",
    "- input type : torch.Tensor\n",
    "- input shape : (N x C x H x W)\n",
    "    - N : batch_size,\n",
    "    - C : channel, (RGB 인 경우 3)\n",
    "    - H : height,\n",
    "    - W : width,\n",
    "    \n",
    "### 1-5 Convolution의 output 크기\n",
    "- Output size = {{input size - filter size + (2*padding)}//Stride} + 1\n",
    "- 예제1)  \n",
    "    input image size : 227x227  \n",
    "    filter size : 11x11  \n",
    "    stride : 4  \n",
    "    padding : 0  \n",
    "    output image size : ???  \n",
    "    - __정답 : 55__\n",
    "   \n",
    "소수점은 버린다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### output 실습1\n",
    "\n",
    "- 예제1)  \n",
    "    input image size : 227x227  \n",
    "    filter size : 11x11  \n",
    "    stride : 4  \n",
    "    padding : 0  \n",
    "    output image size : ???  \n",
    "    - __정답 : 55__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(1, 1, 11, stride = 4, padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 1, kernel_size=(11, 11), stride=(4, 4))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.Tensor(1,1,227,227)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 227, 227])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = conv(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 55, 55])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### output 실습2\n",
    "\n",
    "- 예제1)  \n",
    "    input image size : 32x64  \n",
    "    filter size : 5x5  \n",
    "    stride : 1  \n",
    "    padding : 0  \n",
    "    output image size : ???  \n",
    "    - __정답 : 28x60__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(1, 1, 5, stride=1, padding=0)\n",
    "inputs = torch.Tensor(1,1,32,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = conv(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 60])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-6 Perceptron 과 Convolution\n",
    "\n",
    "그러면 이제부터 지금까지 배워왔던 뉴런(perceptron) 과 Convolution의 관계에 대해 알아 보도록 하겠습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-7. Pooling\n",
    "\n",
    "풀링은 이미지 사이즈를 줄이기 위해 이용하거나 , fully connected 연산을 대체하기 위해 사용되기도 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-8. CNN implementation\n",
    "\n",
    "![](img/20191215_204232.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.Tensor(1,1,28,28)\n",
    "conv1 = nn.Conv2d(1,5,5) # input channel : 1, output channel : 5, filter size : 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = nn.MaxPool2d(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = conv1(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = pool(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 24, 24])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 12, 12])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.Tensor(3,3,128,128)\n",
    "conv1 = nn.Conv2d(3,6,5)\n",
    "pool = nn.MaxPool2d(2)\n",
    "\n",
    "out = conv1(input)\n",
    "out2 = pool(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
